{"cells":[{"cell_type":"code","metadata":{"source_hash":"ed64d553","execution_start":1699332695462,"execution_millis":5050,"deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":0},"deepnote_to_be_reexecuted":false,"cell_id":"a5a0fa2246d0462788f9821f486aaf8c","deepnote_cell_type":"code"},"source":"pip install pycatch22","block_group":"a5a0fa2246d0462788f9821f486aaf8c","execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pycatch22 in /root/venv/lib/python3.9/site-packages (0.4.4)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"source_hash":"f4039807","execution_start":1699332700551,"execution_millis":9284,"deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":0},"deepnote_to_be_reexecuted":false,"cell_id":"65a74f7b348a447bb782eb81b21b6793","deepnote_cell_type":"code"},"source":"pip install sktime\n","block_group":"d80ecf3759a347c49f671836fa90d5cc","execution_count":2,"outputs":[{"name":"stdout","text":"Collecting sktime\n  Downloading sktime-0.24.1-py3-none-any.whl (20.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.7/20.7 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting scikit-base<0.7.0\n  Downloading scikit_base-0.6.1-py3-none-any.whl (122 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pandas<2.2.0,>=1.1 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from sktime) (1.2.5)\nRequirement already satisfied: scikit-learn<1.4.0,>=0.24 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from sktime) (1.1.2)\nRequirement already satisfied: packaging in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from sktime) (21.3)\nRequirement already satisfied: scipy<2.0.0,>=1.2 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from sktime) (1.9.3)\nRequirement already satisfied: numpy<1.27,>=1.21 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from sktime) (1.23.4)\nRequirement already satisfied: python-dateutil>=2.7.3 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from pandas<2.2.0,>=1.1->sktime) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from pandas<2.2.0,>=1.1->sktime) (2022.5)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from scikit-learn<1.4.0,>=0.24->sktime) (3.1.0)\nRequirement already satisfied: joblib>=1.0.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from scikit-learn<1.4.0,>=0.24->sktime) (1.2.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from packaging->sktime) (3.0.9)\nRequirement already satisfied: six>=1.5 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas<2.2.0,>=1.1->sktime) (1.16.0)\nInstalling collected packages: scikit-base, sktime\nSuccessfully installed scikit-base-0.6.1 sktime-0.24.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"source_hash":"13de08a4","execution_start":1699332709845,"execution_millis":6010,"deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":0},"deepnote_to_be_reexecuted":false,"cell_id":"dab0c60a229b473e8b2af886a9e81aed","deepnote_cell_type":"code"},"source":"pip install imbalanced-learn","block_group":"620bee8975d249feb77390efb43fdde5","execution_count":3,"outputs":[{"name":"stdout","text":"Collecting imbalanced-learn\n  Downloading imbalanced_learn-0.11.0-py3-none-any.whl (235 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.6/235.6 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from imbalanced-learn) (1.23.4)\nRequirement already satisfied: scikit-learn>=1.0.2 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from imbalanced-learn) (1.1.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from imbalanced-learn) (3.1.0)\nRequirement already satisfied: joblib>=1.1.1 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from imbalanced-learn) (1.2.0)\nRequirement already satisfied: scipy>=1.5.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from imbalanced-learn) (1.9.3)\nInstalling collected packages: imbalanced-learn\nSuccessfully installed imbalanced-learn-0.11.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":0},"cell_id":"fbe959a7da904784828a1431081d1e59","deepnote_cell_type":"text-cell-h1"},"source":"# Imports","block_group":"7185b6ce7d0d4f768aa671b1c85d6d70"},{"cell_type":"code","metadata":{"source_hash":"b7136fc2","execution_start":1699332715871,"execution_millis":67617,"deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":0},"deepnote_to_be_reexecuted":false,"cell_id":"ea1a0be6f84040c9a0275d54a77fb93a","deepnote_cell_type":"code"},"source":"from collections import Counter\nfrom sklearn.metrics import accuracy_score, matthews_corrcoef, precision_score, recall_score, f1_score, roc_curve, roc_auc_score\nfrom sklearn.svm import OneClassSVM\nfrom sklearn import linear_model\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc\nfrom scipy import stats\nfrom scipy.stats import ks_2samp\nimport matplotlib.pyplot as plt\nimport matplotlib.cbook as cbook\nimport numpy as np\nimport pandas as pd\nimport math\nfrom math import sqrt\nimport statistics \nimport csv\nfrom sklearn.dummy import DummyClassifier\nfrom pycatch22 import catch22_all\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.covariance import EllipticEnvelope\nfrom tensorflow.keras.layers import Input, Dense\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.cluster import AgglomerativeClustering\nfrom scipy.stats import ks_2samp\nfrom scipy.stats import ranksums \nfrom scipy.stats import wasserstein_distance\nfrom scipy.special import rel_entr\nfrom sktime.classification.interval_based import TimeSeriesForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import SMOTE\n\n\n\n%matplotlib inline\n\nfilename0 = '/work/data/Dataset_0_new.csv'\ndata0 = pd.read_csv(filename0)\nprint(\"Number of lines present:\", len(data0))\n\nfilename1 = '/work/data/Dataset_1_new.csv'\ndata1 = pd.read_csv(filename1)\n\nfilename2 = '/work/data/Dataset_2_new.csv'\ndata2 = pd.read_csv(filename2)\n\nfilename3 = '/work/data/Dataset_3_new.csv'\ndata3 = pd.read_csv(filename3)\n\nfilename4 = '/work/data/Dataset_4_new.csv'\ndata4 = pd.read_csv(filename4)\n\nall_data = [data0, data1, data2, data3, data4]\n\nreal_change_points = all_data[0]['Fraud_point'].values\nconsumer_ids = all_data[0]['Customer_ID'].values\n\nprint(consumer_ids)\n","block_group":"ea1a0be6f84040c9a0275d54a77fb93a","execution_count":4,"outputs":[{"name":"stderr","text":"2023-11-07 04:51:59.433544: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-11-07 04:51:59.632876: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n2023-11-07 04:51:59.632933: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n2023-11-07 04:51:59.682865: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2023-11-07 04:52:01.822129: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n2023-11-07 04:52:01.822276: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n2023-11-07 04:52:01.822293: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\nNumber of lines present: 20\n[27396 31709 19373  4691 11612 27966 28310 27391 30199 26952 30894 46170\n 42635  7020 12791 28961 41436 25414 21496 45806]\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":0},"cell_id":"990e13e345974066ae0601904165d839","deepnote_cell_type":"text-cell-h1"},"source":"# Functions","block_group":"146263917e7b4d78a5435560a8d23777"},{"cell_type":"code","metadata":{"source_hash":"e0d60c15","execution_start":1699332783484,"execution_millis":14,"deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":0},"deepnote_to_be_reexecuted":false,"cell_id":"57f4c2810fbf444ebf2e041fa614dc3e","deepnote_cell_type":"code"},"source":"# Get consumer data, condumer ID and fraud point\ndef consumerData(data, index):\n    consumer_data = data.iloc[index,3:].values\n    consumer_id = data['Customer_ID'][index]\n    fraud_point = data['Fraud_point'][index]\n#     print(consumer_data)\n    return consumer_data, consumer_id, fraud_point\n        \ndef setupWeeklyData(consumer_data):\n    day = 96\n    week_period = 7 # number of consecutive days to detect a change\n    week = 7*96\n    year = 96 * 365\n    window_len = day*week_period # window length for the statistical tests\n\n    # Data weekly from first year\n    weekly_data1year = [] # (X_train)\n    # Data weekly from second year\n    weekly_data2year = [] # (X_test)\n\n    for i in range(0, year-(7*96), day): \n        weekly_data1year.append(consumer_data[i:i+window_len])\n        weekly_data2year.append(consumer_data[i+year:(i+year)+window_len])\n        \n    # Convert data into a list\n#     weekly_data1year = np.array(weekly_data1year)\n#     weekly_data2year = np.array(weekly_data2year)\n    return weekly_data1year, weekly_data2year\n\ndef get_ground_truth(num_time_points, fraud_point_week):\n    ground_truth = [0] * num_time_points\n    if 0 <= fraud_point_week < num_time_points:\n        ground_truth[int(fraud_point_week)] = 1\n    ground_truth[int(fraud_point_week):] = [1] * len(ground_truth[int(fraud_point_week):])\n    return ground_truth\n\ndef measure_accuracy(predicted_change_weeks, fraud_point_week):\n    TP = 0\n    FP = 0\n    \n    print(predicted_change_weeks, fraud_point_week)\n    for week in predicted_change_weeks:\n        if abs(week - fraud_point_week) <= 1: # assuming a tolerance of 1 week\n            TP += 1\n        else:\n            FP += 1\n    \n    FN = 0 if TP > 0 else 1  # If we didn't find the fraud point, then it's a False Negative\n    TN = len(predicted_change_weeks) - (TP + FP)  # All other weeks are True Negatives\n    \n    accuracy = (TP + TN) / (TP + TN + FP + FN)\n    if TP + FP != 0:\n        precision = TP / (TP + FP)\n    else:\n        precision = 0\n    recall = TP / (TP + FN)\n    if precision + recall != 0:\n        f1 = 2 * (precision * recall) / (precision + recall)\n    else:\n        f1 = 0\n\n    return accuracy, precision, recall, f1","block_group":"57f4c2810fbf444ebf2e041fa614dc3e","execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":0},"cell_id":"8f65eec2c4934d7cbd773e659d937402","deepnote_cell_type":"text-cell-h1"},"source":"# Stastical","block_group":"cc79de3eb04841e6b58b335de54b1970"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":0},"cell_id":"e19f01b158064fd78082c2efca8871ac","deepnote_cell_type":"text-cell-h2"},"source":"## Wasserstein Distance","block_group":"7acd5425254f447aa4904f3906007a99"},{"cell_type":"code","metadata":{"source_hash":"720474c0","execution_start":1699332783509,"execution_millis":13954,"deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":0},"deepnote_to_be_reexecuted":false,"cell_id":"4dcf1203b3704b9ea90b238b0bcf20a5","deepnote_cell_type":"code"},"source":"total_accuracy = []\ntotal_precision = []\ntotal_recall = []\ntotal_f1 = []\nday = 96\nweek_period = 1\n\ntrue_labels = []\npredicted_labels = []\n\nprint('Wasserstein Distance\\n\\n')\n\ncount = 0\nfor data in all_data:\n    count += 1\n    print(f'Dataset: {count}')\n    for j in range(len(data.index)):\n        consumer_data, consumer_id, fraud_point = consumerData(data, j)\n        fraud_point_day = int((fraud_point/96) - 365)\n        year_counter = 0\n        year_results = []\n        window_len = day*week_period # Window length for the statistical tests\n        metrics_list = []\n        threshold = 0.1\n        fraud_point_week = (fraud_point/96)\n        results = [] # Store the results of the statistical test\n        count = 0 # Counter to verify if the distributions are different during N consecutive days\n        w0 = consumer_data[0:window_len] # set the data of the initial window w0\n        for i in range(0, len(consumer_data), day):\n            w1 = consumer_data[i:i+window_len]\n            distance = wasserstein_distance(w0, w1)\n            results.append(distance)\n                \n            # year_counter = year_counter + 1\n            # if year_counter == 365:\n            #     yearly_mean = statistics.mean(results)\n            #     yearly_sd = statistics.stdev(results)\n            #     new_threshold = yearly_mean * (2*yearly_sd)\n            #     threshold = new_threshold\n            #     month_counter = 0\n        \n        true_labels = [1 if i >= fraud_point_day else 0 for i in range(len(results))]\n        predicted_labels = [1 if w_distance > threshold else 0 for w_distance in results]\n\n        # print(true_labels, '\\n\\n')\n        # print(predicted_labels, '\\n\\n')\n\n        accuracy = accuracy_score(true_labels, predicted_labels)\n        precision = precision_score(true_labels, predicted_labels)\n        recall = recall_score(true_labels, predicted_labels)\n        f1 = f1_score(true_labels, predicted_labels)\n\n        print(f\"Consumer: {consumer_id}\")\n        print(f\"Accuracy: {accuracy:.2f}\")\n        print(f\"Precision: {precision:.2f}\")\n        print(f\"Recall: {recall:.2f}\")\n        print(f\"F1 Score: {f1:.2f}\")\n        print(\"--------------\")\n\n        total_accuracy.append(accuracy)\n        total_precision.append(precision)\n        total_recall.append(recall)\n        total_f1.append(f1)\n\n    print(f'Total accuracy: {sum(total_accuracy)/len(total_accuracy):.4f}', )\n    print(f'Total precision: {sum(total_precision)/len(total_precision):.4f}', )\n    print(f'Total recall: {sum(total_recall)/len(total_recall):.4f}', )\n    print(f'Total f1: {sum(total_f1)/len(total_f1):.4f}', )","block_group":"9fd82068e470495eaf9e6359acfa390d","execution_count":6,"outputs":[{"name":"stdout","text":"Wasserstein Distance\n\n\nDataset: 1\nConsumer: 27396\nAccuracy: 0.87\nPrecision: 0.89\nRecall: 0.98\nF1 Score: 0.93\n--------------\nConsumer: 31709\nAccuracy: 0.86\nPrecision: 0.92\nRecall: 0.92\nF1 Score: 0.92\n--------------\nConsumer: 19373\nAccuracy: 0.86\nPrecision: 0.86\nRecall: 1.00\nF1 Score: 0.93\n--------------\nConsumer: 4691\nAccuracy: 0.97\nPrecision: 0.99\nRecall: 0.98\nF1 Score: 0.98\n--------------\nConsumer: 11612\nAccuracy: 0.93\nPrecision: 0.96\nRecall: 0.97\nF1 Score: 0.96\n--------------\nConsumer: 27966\nAccuracy: 0.78\nPrecision: 0.87\nRecall: 0.88\nF1 Score: 0.88\n--------------\nConsumer: 28310\nAccuracy: 0.94\nPrecision: 0.95\nRecall: 0.99\nF1 Score: 0.97\n--------------\nConsumer: 27391\nAccuracy: 0.96\nPrecision: 0.98\nRecall: 0.98\nF1 Score: 0.98\n--------------\nConsumer: 30199\nAccuracy: 0.87\nPrecision: 0.89\nRecall: 0.96\nF1 Score: 0.93\n--------------\nConsumer: 26952\nAccuracy: 0.78\nPrecision: 0.80\nRecall: 0.97\nF1 Score: 0.88\n--------------\nConsumer: 30894\nAccuracy: 0.97\nPrecision: 0.98\nRecall: 0.99\nF1 Score: 0.98\n--------------\nConsumer: 46170\nAccuracy: 0.95\nPrecision: 0.95\nRecall: 1.00\nF1 Score: 0.97\n--------------\nConsumer: 42635\nAccuracy: 0.95\nPrecision: 0.95\nRecall: 0.99\nF1 Score: 0.97\n--------------\nConsumer: 7020\nAccuracy: 0.80\nPrecision: 0.80\nRecall: 0.99\nF1 Score: 0.89\n--------------\nConsumer: 12791\nAccuracy: 0.90\nPrecision: 0.90\nRecall: 1.00\nF1 Score: 0.95\n--------------\nConsumer: 28961\nAccuracy: 0.61\nPrecision: 0.62\nRecall: 0.95\nF1 Score: 0.75\n--------------\nConsumer: 41436\nAccuracy: 0.88\nPrecision: 0.88\nRecall: 0.99\nF1 Score: 0.93\n--------------\nConsumer: 25414\nAccuracy: 0.98\nPrecision: 0.99\nRecall: 0.98\nF1 Score: 0.99\n--------------\nConsumer: 21496\nAccuracy: 0.60\nPrecision: 0.61\nRecall: 0.99\nF1 Score: 0.75\n--------------\nConsumer: 45806\nAccuracy: 0.91\nPrecision: 0.95\nRecall: 0.95\nF1 Score: 0.95\n--------------\nTotal accuracy: 0.8685\nTotal precision: 0.8872\nTotal recall: 0.9735\nTotal f1: 0.9246\nDataset: 1\nConsumer: 27396\nAccuracy: 0.69\nPrecision: 0.70\nRecall: 0.98\nF1 Score: 0.81\n--------------\nConsumer: 19373\nAccuracy: 0.62\nPrecision: 0.62\nRecall: 1.00\nF1 Score: 0.76\n--------------\nConsumer: 10042\nAccuracy: 0.70\nPrecision: 0.70\nRecall: 0.99\nF1 Score: 0.82\n--------------\nConsumer: 21532\nAccuracy: 0.54\nPrecision: 0.54\nRecall: 0.99\nF1 Score: 0.70\n--------------\nConsumer: 18786\nAccuracy: 0.78\nPrecision: 0.78\nRecall: 0.99\nF1 Score: 0.87\n--------------\nConsumer: 46170\nAccuracy: 0.83\nPrecision: 0.83\nRecall: 1.00\nF1 Score: 0.91\n--------------\nConsumer: 28639\nAccuracy: 0.59\nPrecision: 0.59\nRecall: 0.99\nF1 Score: 0.74\n--------------\nConsumer: 41383\nAccuracy: 0.89\nPrecision: 0.97\nRecall: 0.91\nF1 Score: 0.94\n--------------\nConsumer: 48747\nAccuracy: 0.63\nPrecision: 0.65\nRecall: 0.92\nF1 Score: 0.77\n--------------\nConsumer: 36438\nAccuracy: 0.75\nPrecision: 0.78\nRecall: 0.95\nF1 Score: 0.85\n--------------\nConsumer: 34715\nAccuracy: 0.68\nPrecision: 0.69\nRecall: 0.98\nF1 Score: 0.81\n--------------\nConsumer: 24746\nAccuracy: 0.71\nPrecision: 0.71\nRecall: 1.00\nF1 Score: 0.83\n--------------\nConsumer: 46526\nAccuracy: 0.98\nPrecision: 0.99\nRecall: 0.98\nF1 Score: 0.99\n--------------\nConsumer: 18245\nAccuracy: 0.65\nPrecision: 0.66\nRecall: 0.95\nF1 Score: 0.78\n--------------\nConsumer: 8355\nAccuracy: 0.98\nPrecision: 0.99\nRecall: 0.99\nF1 Score: 0.99\n--------------\nConsumer: 21496\nAccuracy: 0.94\nPrecision: 0.94\nRecall: 0.99\nF1 Score: 0.97\n--------------\nConsumer: 14105\nAccuracy: 0.62\nPrecision: 0.62\nRecall: 0.98\nF1 Score: 0.76\n--------------\nConsumer: 6465\nAccuracy: 0.58\nPrecision: 0.59\nRecall: 0.94\nF1 Score: 0.73\n--------------\nConsumer: 27095\nAccuracy: 0.78\nPrecision: 0.86\nRecall: 0.88\nF1 Score: 0.87\n--------------\nConsumer: 6770\nAccuracy: 0.55\nPrecision: 0.55\nRecall: 0.98\nF1 Score: 0.70\n--------------\nTotal accuracy: 0.7963\nTotal precision: 0.8128\nTotal recall: 0.9719\nTotal f1: 0.8775\nDataset: 1\nConsumer: 27396\nAccuracy: 0.96\nPrecision: 0.98\nRecall: 0.97\nF1 Score: 0.98\n--------------\nConsumer: 23361\nAccuracy: 0.59\nPrecision: 0.59\nRecall: 0.97\nF1 Score: 0.74\n--------------\nConsumer: 3832\nAccuracy: 0.60\nPrecision: 0.60\nRecall: 1.00\nF1 Score: 0.75\n--------------\nConsumer: 20985\nAccuracy: 0.84\nPrecision: 0.85\nRecall: 0.99\nF1 Score: 0.92\n--------------\nConsumer: 18786\nAccuracy: 0.59\nPrecision: 0.60\nRecall: 0.99\nF1 Score: 0.74\n--------------\nConsumer: 12636\nAccuracy: 0.98\nPrecision: 0.98\nRecall: 1.00\nF1 Score: 0.99\n--------------\nConsumer: 30894\nAccuracy: 0.59\nPrecision: 0.59\nRecall: 0.98\nF1 Score: 0.74\n--------------\nConsumer: 1318\nAccuracy: 0.58\nPrecision: 0.59\nRecall: 0.96\nF1 Score: 0.73\n--------------\nConsumer: 543\nAccuracy: 0.83\nPrecision: 0.88\nRecall: 0.94\nF1 Score: 0.91\n--------------\nConsumer: 4011\nAccuracy: 0.85\nPrecision: 0.88\nRecall: 0.96\nF1 Score: 0.92\n--------------\nConsumer: 38899\nAccuracy: 0.82\nPrecision: 0.85\nRecall: 0.95\nF1 Score: 0.90\n--------------\nConsumer: 7020\nAccuracy: 0.65\nPrecision: 0.65\nRecall: 0.99\nF1 Score: 0.79\n--------------\nConsumer: 8189\nAccuracy: 0.61\nPrecision: 0.61\nRecall: 0.96\nF1 Score: 0.75\n--------------\nConsumer: 20043\nAccuracy: 0.74\nPrecision: 0.78\nRecall: 0.93\nF1 Score: 0.85\n--------------\nConsumer: 27171\nAccuracy: 0.80\nPrecision: 0.85\nRecall: 0.93\nF1 Score: 0.89\n--------------\nConsumer: 27439\nAccuracy: 0.50\nPrecision: 0.56\nRecall: 0.70\nF1 Score: 0.62\n--------------\nConsumer: 21496\nAccuracy: 0.58\nPrecision: 0.59\nRecall: 0.98\nF1 Score: 0.74\n--------------\nConsumer: 28544\nAccuracy: 0.84\nPrecision: 0.85\nRecall: 0.99\nF1 Score: 0.91\n--------------\nConsumer: 7212\nAccuracy: 0.75\nPrecision: 0.79\nRecall: 0.94\nF1 Score: 0.85\n--------------\nConsumer: 10656\nAccuracy: 0.65\nPrecision: 0.65\nRecall: 0.98\nF1 Score: 0.78\n--------------\nTotal accuracy: 0.7700\nTotal precision: 0.7871\nTotal recall: 0.9665\nTotal f1: 0.8596\nDataset: 1\nConsumer: 25096\nAccuracy: 0.94\nPrecision: 0.94\nRecall: 1.00\nF1 Score: 0.97\n--------------\nConsumer: 19373\nAccuracy: 0.53\nPrecision: 0.53\nRecall: 1.00\nF1 Score: 0.69\n--------------\nConsumer: 23361\nAccuracy: 0.95\nPrecision: 0.97\nRecall: 0.98\nF1 Score: 0.97\n--------------\nConsumer: 29752\nAccuracy: 0.57\nPrecision: 0.56\nRecall: 0.99\nF1 Score: 0.72\n--------------\nConsumer: 27966\nAccuracy: 0.60\nPrecision: 0.66\nRecall: 0.85\nF1 Score: 0.75\n--------------\nConsumer: 1091\nAccuracy: 0.60\nPrecision: 0.60\nRecall: 0.98\nF1 Score: 0.75\n--------------\nConsumer: 18786\nAccuracy: 0.94\nPrecision: 0.95\nRecall: 0.99\nF1 Score: 0.97\n--------------\nConsumer: 42635\nAccuracy: 0.61\nPrecision: 0.61\nRecall: 0.99\nF1 Score: 0.76\n--------------\nConsumer: 543\nAccuracy: 0.74\nPrecision: 0.78\nRecall: 0.92\nF1 Score: 0.85\n--------------\nConsumer: 3799\nAccuracy: 0.88\nPrecision: 0.91\nRecall: 0.95\nF1 Score: 0.93\n--------------\nConsumer: 32895\nAccuracy: 0.93\nPrecision: 0.96\nRecall: 0.96\nF1 Score: 0.96\n--------------\nConsumer: 36438\nAccuracy: 0.55\nPrecision: 0.55\nRecall: 0.97\nF1 Score: 0.70\n--------------\nConsumer: 26637\nAccuracy: 0.53\nPrecision: 0.53\nRecall: 0.99\nF1 Score: 0.69\n--------------\nConsumer: 678\nAccuracy: 0.59\nPrecision: 0.65\nRecall: 0.83\nF1 Score: 0.73\n--------------\nConsumer: 31543\nAccuracy: 0.87\nPrecision: 0.87\nRecall: 1.00\nF1 Score: 0.93\n--------------\nConsumer: 1209\nAccuracy: 0.78\nPrecision: 0.79\nRecall: 0.99\nF1 Score: 0.88\n--------------\nConsumer: 18828\nAccuracy: 0.87\nPrecision: 0.89\nRecall: 0.97\nF1 Score: 0.93\n--------------\nConsumer: 45806\nAccuracy: 0.67\nPrecision: 0.67\nRecall: 0.99\nF1 Score: 0.80\n--------------\nConsumer: 10656\nAccuracy: 0.87\nPrecision: 0.88\nRecall: 0.99\nF1 Score: 0.93\n--------------\nConsumer: 2440\nAccuracy: 0.68\nPrecision: 0.68\nRecall: 1.00\nF1 Score: 0.81\n--------------\nTotal accuracy: 0.7611\nTotal precision: 0.7777\nTotal recall: 0.9665\nTotal f1: 0.8534\nDataset: 1\nConsumer: 19373\nAccuracy: 0.70\nPrecision: 0.70\nRecall: 1.00\nF1 Score: 0.83\n--------------\nConsumer: 31319\nAccuracy: 0.67\nPrecision: 0.70\nRecall: 0.93\nF1 Score: 0.80\n--------------\nConsumer: 10042\nAccuracy: 0.63\nPrecision: 0.63\nRecall: 0.99\nF1 Score: 0.77\n--------------\nConsumer: 38452\nAccuracy: 0.88\nPrecision: 0.88\nRecall: 1.00\nF1 Score: 0.93\n--------------\nConsumer: 26952\nAccuracy: 0.82\nPrecision: 0.83\nRecall: 0.99\nF1 Score: 0.90\n--------------\nConsumer: 1318\nAccuracy: 0.95\nPrecision: 0.98\nRecall: 0.98\nF1 Score: 0.98\n--------------\nConsumer: 32895\nAccuracy: 0.56\nPrecision: 0.56\nRecall: 0.98\nF1 Score: 0.71\n--------------\nConsumer: 7020\nAccuracy: 0.57\nPrecision: 0.57\nRecall: 0.99\nF1 Score: 0.72\n--------------\nConsumer: 49438\nAccuracy: 0.88\nPrecision: 0.88\nRecall: 1.00\nF1 Score: 0.94\n--------------\nConsumer: 678\nAccuracy: 0.55\nPrecision: 0.61\nRecall: 0.82\nF1 Score: 0.70\n--------------\nConsumer: 78\nAccuracy: 0.87\nPrecision: 0.91\nRecall: 0.95\nF1 Score: 0.93\n--------------\nConsumer: 5932\nAccuracy: 0.82\nPrecision: 0.84\nRecall: 0.97\nF1 Score: 0.90\n--------------\nConsumer: 48264\nAccuracy: 0.71\nPrecision: 0.71\nRecall: 0.98\nF1 Score: 0.83\n--------------\nConsumer: 27171\nAccuracy: 0.92\nPrecision: 0.98\nRecall: 0.94\nF1 Score: 0.96\n--------------\nConsumer: 2220\nAccuracy: 0.83\nPrecision: 0.83\nRecall: 0.99\nF1 Score: 0.91\n--------------\nConsumer: 21496\nAccuracy: 0.82\nPrecision: 0.83\nRecall: 0.99\nF1 Score: 0.90\n--------------\nConsumer: 28544\nAccuracy: 0.88\nPrecision: 0.89\nRecall: 0.99\nF1 Score: 0.94\n--------------\nConsumer: 14105\nAccuracy: 0.95\nPrecision: 0.96\nRecall: 0.99\nF1 Score: 0.97\n--------------\nConsumer: 43355\nAccuracy: 0.71\nPrecision: 0.71\nRecall: 1.00\nF1 Score: 0.83\n--------------\nConsumer: 45806\nAccuracy: 0.95\nPrecision: 0.96\nRecall: 0.99\nF1 Score: 0.98\n--------------\nTotal accuracy: 0.7656\nTotal precision: 0.7817\nTotal recall: 0.9678\nTotal f1: 0.8569\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":0},"cell_id":"aa123b56c4bf4479bde0d3e6185e3ac3","deepnote_cell_type":"text-cell-h1"},"source":"# Catch 22 - ML","block_group":"71ad05a1eba849cfa31a0f17f4f68562"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":0},"cell_id":"ea62ef69e8d34f388210aee3e8663282","deepnote_cell_type":"text-cell-p"},"source":"","block_group":"43e461433a894b7497218c13265d993d"},{"cell_type":"markdown","metadata":{"is_collapsed":false,"formattedRanges":[],"deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":0},"cell_id":"e89038047f4e423399f15fcb2d26d7fc","deepnote_cell_type":"text-cell-h2"},"source":"## Isolation Forest","block_group":"78f83a99390d45f291a31c2c7269a053"},{"cell_type":"code","metadata":{"source_hash":"94fc85e2","execution_start":1699332797499,"execution_millis":307672,"deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":0},"deepnote_to_be_reexecuted":false,"cell_id":"a95e8ecc6209468b82dc0524c3a3b610","deepnote_cell_type":"code"},"source":"\n# Create a StandardScaler instance\nscaler = StandardScaler()\n\n# Deciding to keep the first N principal components\nN_COMPONENTS = 22\npca = PCA(n_components=N_COMPONENTS)\n\ntotal_accuracy = []\ntotal_precision = []\ntotal_recall = []\ntotal_f1 = []\nisolation_pred = []\n\nprint('Isolation Forest\\n\\n')\n\ncount = 0\nfor i in all_data:\n    count += 1\n    print(f'Dataset: {count}')\n    for j in range(len(i.index)):\n        consumer_data, consumer_id, fraud_point = consumerData(i, j)\n        fraud_point_week = (fraud_point/96)\n        weekly_consumption_year1, weekly_consumption_year2 = setupWeeklyData(consumer_data)\n\n        features = []\n        for week_data in weekly_consumption_year1 + weekly_consumption_year2: # Summing up to use features\n            result = catch22_all(week_data)\n            features.append(result['values'])\n\n        features = np.array(features)\n\n        # Check and handle NaN values \n        features = features[~np.isnan(features).any(axis=1)]\n\n        # Normalize the features\n        features = scaler.fit_transform(features)\n\n        # Applying PCA transformation\n        features = pca.fit_transform(features)\n\n        smote = SMOTE(random_state=42)\n\n        # Aplicar o over-sampling ao conjunto de treinamento\n        features = smote.fit_resample(features)\n        \n        isf = IsolationForest(max_samples=80, random_state=2, n_estimators=50, contamination=float(0.1))\n        isf.fit(features)\n        pred = isf.predict(features)\n\n        # Convert predictions to match ground truth format: 1 for anomalies and 0 for normal\n        pred[pred == 1] = 0\n        pred[pred == -1] = 1\n\n        # Get the ground truth\n        ground_truth = get_ground_truth(len(features), fraud_point_week)\n\n        # Calculate accuracy measures\n        pred = pred.tolist()\n        isolation_pred = pred\n        # print(\"Pred: \", pred, \"ground truth: \", ground_truth)\n\n        accuracy = accuracy_score(ground_truth, pred)\n        precision = precision_score(ground_truth, pred)\n        recall = recall_score(ground_truth, pred)\n        f1 = f1_score(ground_truth, pred)\n\n        print(f\"Consumer: {consumer_id}\")\n        print(f\"Accuracy: {accuracy:.2f}\")\n        print(f\"Precision: {precision:.2f}\")\n        print(f\"Recall: {recall:.2f}\")\n        print(f\"F1 Score: {f1:.2f}\")\n        print(\"--------------\")\n\n        total_accuracy.append(accuracy)\n        total_precision.append(precision)\n        total_recall.append(recall)\n        total_f1.append(f1)\n\n        num_time_points = len(features)\n        num_features = features.shape[1]\n\n\n    print(f'Total accuracy: {sum(total_accuracy)/len(total_accuracy):.4f}', )\n    print(f'Total precision: {sum(total_precision)/len(total_precision):.4f}', )\n    print(f'Total recall: {sum(total_recall)/len(total_recall):.4f}', )\n    print(f'Total f1: {sum(total_f1)/len(total_f1):.4f}', )","block_group":"a95e8ecc6209468b82dc0524c3a3b610","execution_count":null,"outputs":[{"name":"stdout","text":"Isolation Forest\n\n\nDataset: 1\nConsumer: 27396\nAccuracy: 0.63\nPrecision: 0.51\nRecall: 0.14\nF1 Score: 0.22\n--------------\nConsumer: 31709\nAccuracy: 0.60\nPrecision: 0.36\nRecall: 0.10\nF1 Score: 0.15\n--------------\nConsumer: 19373\nAccuracy: 0.64\nPrecision: 0.42\nRecall: 0.12\nF1 Score: 0.19\n--------------\nConsumer: 4691\nAccuracy: 0.59\nPrecision: 0.86\nRecall: 0.18\nF1 Score: 0.30\n--------------\nConsumer: 11612\nAccuracy: 0.53\nPrecision: 0.36\nRecall: 0.08\nF1 Score: 0.13\n--------------\nConsumer: 27966\nAccuracy: 0.67\nPrecision: 0.57\nRecall: 0.16\nF1 Score: 0.26\n--------------\nConsumer: 28310\nAccuracy: 0.59\nPrecision: 0.42\nRecall: 0.11\nF1 Score: 0.17\n--------------\nConsumer: 27391\nAccuracy: 0.47\nPrecision: 0.15\nRecall: 0.03\nF1 Score: 0.05\n--------------\nConsumer: 30199\nAccuracy: 0.58\nPrecision: 0.22\nRecall: 0.06\nF1 Score: 0.10\n--------------\nConsumer: 26952\nAccuracy: 0.62\nPrecision: 0.03\nRecall: 0.01\nF1 Score: 0.01\n--------------\nConsumer: 30894\nAccuracy: 0.57\nPrecision: 0.68\nRecall: 0.15\nF1 Score: 0.24\n--------------\nConsumer: 46170\nAccuracy: 0.53\nPrecision: 0.33\nRecall: 0.08\nF1 Score: 0.12\n--------------\nConsumer: 42635\nAccuracy: 0.55\nPrecision: 0.46\nRecall: 0.11\nF1 Score: 0.17\n--------------\nConsumer: 7020\nAccuracy: 0.78\nPrecision: 0.83\nRecall: 0.29\nF1 Score: 0.43\n--------------\nConsumer: 12791\nAccuracy: 0.59\nPrecision: 0.39\nRecall: 0.10\nF1 Score: 0.16\n--------------\nConsumer: 28961\nAccuracy: 0.84\nPrecision: 0.14\nRecall: 0.16\nF1 Score: 0.15\n--------------\nConsumer: 41436\nAccuracy: 0.68\nPrecision: 0.72\nRecall: 0.20\nF1 Score: 0.31\n--------------\nConsumer: 25414\nAccuracy: 0.45\nPrecision: 0.14\nRecall: 0.03\nF1 Score: 0.05\n--------------\nConsumer: 21496\nAccuracy: 0.88\nPrecision: 0.36\nRecall: 0.41\nF1 Score: 0.39\n--------------\nConsumer: 45806\nAccuracy: 0.60\nPrecision: 0.72\nRecall: 0.16\nF1 Score: 0.27\n--------------\nTotal accuracy: 0.6196\nTotal precision: 0.4340\nTotal recall: 0.1340\nTotal f1: 0.1937\nDataset: 2\nConsumer: 27396\nAccuracy: 0.84\nPrecision: 0.53\nRecall: 0.32\nF1 Score: 0.40\n--------------\nConsumer: 19373\nAccuracy: 0.94\nPrecision: 0.67\nRecall: 0.69\nF1 Score: 0.68\n--------------\nConsumer: 10042\nAccuracy: 0.82\nPrecision: 0.54\nRecall: 0.29\nF1 Score: 0.38\n--------------\nConsumer: 21532\nAccuracy: 0.92\nPrecision: 0.21\nRecall: 0.83\nF1 Score: 0.33\n--------------\nConsumer: 18786\nAccuracy: 0.82\nPrecision: 0.94\nRecall: 0.36\nF1 Score: 0.52\n--------------\nConsumer: 46170\nAccuracy: 0.70\nPrecision: 0.60\nRecall: 0.19\nF1 Score: 0.29\n--------------\nConsumer: 28639\nAccuracy: 0.83\nPrecision: 0.01\nRecall: 0.02\nF1 Score: 0.02\n--------------\nConsumer: 41383\nAccuracy: 0.57\nPrecision: 0.50\nRecall: 0.12\nF1 Score: 0.19\n--------------\nConsumer: 48747\nAccuracy: 0.79\nPrecision: 0.18\nRecall: 0.13\nF1 Score: 0.15\n--------------\nConsumer: 36438\nAccuracy: 0.68\nPrecision: 0.22\nRecall: 0.08\nF1 Score: 0.12\n--------------\nConsumer: 34715\nAccuracy: 0.77\nPrecision: 0.19\nRecall: 0.12\nF1 Score: 0.15\n--------------\nConsumer: 24746\nAccuracy: 0.73\nPrecision: 0.11\nRecall: 0.06\nF1 Score: 0.08\n--------------\nConsumer: 46526\nAccuracy: 0.53\nPrecision: 0.49\nRecall: 0.11\nF1 Score: 0.17\n--------------\nConsumer: 18245\nAccuracy: 0.83\nPrecision: 0.39\nRecall: 0.27\nF1 Score: 0.32\n--------------\nConsumer: 8355\nAccuracy: 0.44\nPrecision: 0.11\nRecall: 0.02\nF1 Score: 0.04\n--------------\nConsumer: 21496\nAccuracy: 0.53\nPrecision: 0.32\nRecall: 0.07\nF1 Score: 0.12\n--------------\nConsumer: 14105\nAccuracy: 0.89\nPrecision: 0.44\nRecall: 0.46\nF1 Score: 0.45\n--------------\nConsumer: 6465\nAccuracy: 0.84\nPrecision: 0.08\nRecall: 0.11\nF1 Score: 0.10\n--------------\nConsumer: 27095\nAccuracy: 0.74\nPrecision: 0.76\nRecall: 0.24\nF1 Score: 0.37\n--------------\nConsumer: 6770\nAccuracy: 0.90\nPrecision: 0.14\nRecall: 0.56\nF1 Score: 0.22\n--------------\nTotal accuracy: 0.6879\nTotal precision: 0.4033\nTotal recall: 0.1930\nTotal f1: 0.2239\nDataset: 3\nConsumer: 27396\nAccuracy: 0.58\nPrecision: 0.72\nRecall: 0.16\nF1 Score: 0.26\n--------------\nConsumer: 23361\nAccuracy: 0.86\nPrecision: 0.15\nRecall: 0.21\nF1 Score: 0.18\n--------------\nConsumer: 3832\nAccuracy: 0.84\nPrecision: 0.10\nRecall: 0.13\nF1 Score: 0.11\n--------------\nConsumer: 20985\nAccuracy: 0.63\nPrecision: 0.38\nRecall: 0.11\nF1 Score: 0.17\n--------------\nConsumer: 18786\nAccuracy: 0.95\nPrecision: 0.62\nRecall: 0.83\nF1 Score: 0.71\n--------------\nConsumer: 12636\nAccuracy: 0.48\nPrecision: 0.25\nRecall: 0.05\nF1 Score: 0.09\n--------------\nConsumer: 30894\nAccuracy: 0.91\nPrecision: 0.39\nRecall: 0.56\nF1 Score: 0.46\n--------------\nConsumer: 1318\nAccuracy: 0.84\nPrecision: 0.08\nRecall: 0.11\nF1 Score: 0.09\n--------------\nConsumer: 543\nAccuracy: 0.66\nPrecision: 0.61\nRecall: 0.17\nF1 Score: 0.26\n--------------\nConsumer: 4011\nAccuracy: 0.58\nPrecision: 0.25\nRecall: 0.07\nF1 Score: 0.11\n--------------\nConsumer: 38899\nAccuracy: 0.63\nPrecision: 0.33\nRecall: 0.10\nF1 Score: 0.15\n--------------\nConsumer: 7020\nAccuracy: 0.87\nPrecision: 0.51\nRecall: 0.39\nF1 Score: 0.44\n--------------\nConsumer: 8189\nAccuracy: 0.91\nPrecision: 0.40\nRecall: 0.54\nF1 Score: 0.46\n--------------\nConsumer: 20043\nAccuracy: 0.75\nPrecision: 0.62\nRecall: 0.23\nF1 Score: 0.34\n--------------\nConsumer: 27171\nAccuracy: 0.59\nPrecision: 0.14\nRecall: 0.04\nF1 Score: 0.06\n--------------\nConsumer: 27439\nAccuracy: 0.88\nPrecision: 0.25\nRecall: 0.36\nF1 Score: 0.30\n--------------\nConsumer: 21496\nAccuracy: 0.91\nPrecision: 0.42\nRecall: 0.58\nF1 Score: 0.48\n--------------\nConsumer: 28544\nAccuracy: 0.66\nPrecision: 0.49\nRecall: 0.15\nF1 Score: 0.23\n--------------\nConsumer: 7212\nAccuracy: 0.71\nPrecision: 0.40\nRecall: 0.15\nF1 Score: 0.22\n--------------\nConsumer: 10656\nAccuracy: 0.78\nPrecision: 0.06\nRecall: 0.04\nF1 Score: 0.05\n--------------\nTotal accuracy: 0.7089\nTotal precision: 0.3885\nTotal recall: 0.2115\nTotal f1: 0.2354\nDataset: 4\nConsumer: 25096\nAccuracy: 0.54\nPrecision: 0.35\nRecall: 0.08\nF1 Score: 0.13\n--------------\nConsumer: 19373\nAccuracy: 0.91\nPrecision: 0.07\nRecall: 0.83\nF1 Score: 0.13\n--------------\nConsumer: 23361\nAccuracy: 0.49\nPrecision: 0.21\nRecall: 0.05\nF1 Score: 0.08\n--------------\nConsumer: 29752\nAccuracy: 0.89\nPrecision: 0.10\nRecall: 0.32\nF1 Score: 0.15\n--------------\nConsumer: 27966\nAccuracy: 0.91\nPrecision: 0.88\nRecall: 0.53\nF1 Score: 0.66\n--------------\nConsumer: 1091\nAccuracy: 0.84\nPrecision: 0.17\nRecall: 0.19\nF1 Score: 0.18\n--------------\nConsumer: 18786\nAccuracy: 0.66\nPrecision: 0.93\nRecall: 0.22\nF1 Score: 0.35\n--------------\nConsumer: 42635\nAccuracy: 0.83\nPrecision: 0.10\nRecall: 0.11\nF1 Score: 0.10\n--------------\nConsumer: 543\nAccuracy: 0.69\nPrecision: 0.26\nRecall: 0.10\nF1 Score: 0.15\n--------------\nConsumer: 3799\nAccuracy: 0.67\nPrecision: 0.61\nRecall: 0.17\nF1 Score: 0.27\n--------------\nConsumer: 32895\nAccuracy: 0.57\nPrecision: 0.62\nRecall: 0.14\nF1 Score: 0.23\n--------------\nConsumer: 36438\nAccuracy: 0.92\nPrecision: 0.28\nRecall: 0.91\nF1 Score: 0.43\n--------------\nConsumer: 26637\nAccuracy: 0.91\nPrecision: 0.07\nRecall: 0.83\nF1 Score: 0.13\n--------------\nConsumer: 678\nAccuracy: 0.91\nPrecision: 0.81\nRecall: 0.54\nF1 Score: 0.65\n--------------\nConsumer: 31543\nAccuracy: 0.66\nPrecision: 0.58\nRecall: 0.16\nF1 Score: 0.26\n--------------\nConsumer: 1209\nAccuracy: 0.76\nPrecision: 0.64\nRecall: 0.24\nF1 Score: 0.35\n--------------\nConsumer: 18828\nAccuracy: 0.71\nPrecision: 0.88\nRecall: 0.24\nF1 Score: 0.38\n--------------\nConsumer: 45806\nAccuracy: 0.79\nPrecision: 0.21\nRecall: 0.14\nF1 Score: 0.17\n--------------\nConsumer: 10656\nAccuracy: 0.56\nPrecision: 0.14\nRecall: 0.04\nF1 Score: 0.06\n--------------\nConsumer: 2440\nAccuracy: 0.77\nPrecision: 0.19\nRecall: 0.12\nF1 Score: 0.15\n--------------\nTotal accuracy: 0.7191\nTotal precision: 0.3924\nTotal recall: 0.2332\nTotal f1: 0.2389\nDataset: 5\nConsumer: 19373\nAccuracy: 0.82\nPrecision: 0.54\nRecall: 0.29\nF1 Score: 0.38\n--------------\nConsumer: 31319\nAccuracy: 0.78\nPrecision: 0.35\nRecall: 0.19\nF1 Score: 0.24\n--------------\nConsumer: 10042\nAccuracy: 0.79\nPrecision: 0.03\nRecall: 0.02\nF1 Score: 0.03\n--------------\nConsumer: 38452\nAccuracy: 0.66\nPrecision: 0.64\nRecall: 0.17\nF1 Score: 0.27\n--------------\nConsumer: 26952\nAccuracy: 0.61\nPrecision: 0.11\nRecall: 0.04\nF1 Score: 0.05\n--------------\nConsumer: 1318\nAccuracy: 0.56\nPrecision: 0.62\nRecall: 0.14\nF1 Score: 0.22\n--------------\nConsumer: 32895\nAccuracy: 0.92\nPrecision: 0.33\nRecall: 0.71\nF1 Score: 0.45\n--------------\nConsumer: 7020\nAccuracy: 0.93\nPrecision: 0.39\nRecall: 0.82\nF1 Score: 0.53\n--------------\nConsumer: 49438\nAccuracy: 0.56\nPrecision: 0.12\nRecall: 0.03\nF1 Score: 0.05\n--------------\nConsumer: 678\nAccuracy: 0.96\nPrecision: 0.86\nRecall: 0.76\nF1 Score: 0.81\n--------------\nConsumer: 78\nAccuracy: 0.57\nPrecision: 0.22\nRecall: 0.06\nF1 Score: 0.09\n--------------\nConsumer: 5932\nAccuracy: 0.64\nPrecision: 0.28\nRecall: 0.09\nF1 Score: 0.13\n--------------\nConsumer: 48264\nAccuracy: 0.76\nPrecision: 0.26\nRecall: 0.14\nF1 Score: 0.18\n--------------\nConsumer: 27171\nAccuracy: 0.46\nPrecision: 0.11\nRecall: 0.02\nF1 Score: 0.04\n--------------\nConsumer: 2220\nAccuracy: 0.73\nPrecision: 0.71\nRecall: 0.23\nF1 Score: 0.34\n--------------\nConsumer: 21496\nAccuracy: 0.79\nPrecision: 1.00\nRecall: 0.32\nF1 Score: 0.49\n--------------\nConsumer: 28544\nAccuracy: 0.60\nPrecision: 0.40\nRecall: 0.11\nF1 Score: 0.17\n--------------\nConsumer: 14105\nAccuracy: 0.54\nPrecision: 0.43\nRecall: 0.10\nF1 Score: 0.16\n--------------\nConsumer: 43355\nAccuracy: 0.74\nPrecision: 0.18\nRecall: 0.09\nF1 Score: 0.12\n--------------\nConsumer: 45806\nAccuracy: 0.57\nPrecision: 0.57\nRecall: 0.13\nF1 Score: 0.21\n--------------\nTotal accuracy: 0.7150\nTotal precision: 0.3956\nTotal recall: 0.2311\nTotal f1: 0.2409\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"source_hash":"682d43a8","execution_start":1699248398818,"execution_millis":22,"deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":0},"deepnote_to_be_reexecuted":false,"cell_id":"0031b14f7de348da867e3d9f07fb68fc","deepnote_cell_type":"code"},"source":"IsolationForest","block_group":"0031b14f7de348da867e3d9f07fb68fc","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":0},"cell_id":"6790e83956774423aac1b23dc9d85c3f","deepnote_cell_type":"text-cell-h2"},"source":"## One Class SVM ","block_group":"a3b7a2b9aae3421a990eacd824556646"},{"cell_type":"code","metadata":{"source_hash":"649c23c6","execution_start":1699248398834,"execution_millis":289598,"deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":0},"deepnote_to_be_reexecuted":false,"cell_id":"d646f6109f684ed2be04b39f6c3e7707","deepnote_cell_type":"code"},"source":"total_accuracy = []\ntotal_precision = []\ntotal_recall = []\ntotal_f1 = []\noneclass_pred = []\n\nprint('One Class SVM\\n\\n')\n\ncount = 0\nfor i in all_data:\n    count += 1\n    print(f'Dataset: {count}')\n    for j in range(len(i.index)):\n        consumer_data, consumer_id, fraud_point = consumerData(i, j)\n        fraud_point_week = (fraud_point/96)\n        weekly_consumption_year1, weekly_consumption_year2 = setupWeeklyData(consumer_data)\n\n        features = []\n        for week_data in weekly_consumption_year1 + weekly_consumption_year2:\n            result = catch22_all(week_data)\n            features.append(result['values'])\n\n        features = np.array(features)\n\n        # Check and handle NaN values \n        features = features[~np.isnan(features).any(axis=1)]\n\n        # Normalize the features\n        features = scaler.fit_transform(features)\n\n        # Applying PCA transformation\n        features = pca.fit_transform(features)\n\n         # Initialize OneClassSVM\n        clf = OneClassSVM(kernel=\"rbf\", nu=0.3)\n        clf.fit(features)\n\n        # Predict the anomalies\n        pred = clf.predict(features)\n\n        # Convert predictions to match ground truth format: 1 for anomalies and 0 for normal\n        pred[pred == 1] = 0\n        pred[pred == -1] = 1\n\n        # Get the ground truth\n        ground_truth = get_ground_truth(len(features), fraud_point_week)\n\n        # Calculate accuracy measures\n        pred = pred.tolist()\n        oneclass_pred = pred\n        # print(\"Pred: \", pred, \"ground truth: \", ground_truth)\n\n        accuracy = accuracy_score(ground_truth, pred)\n        precision = precision_score(ground_truth, pred)\n        recall = recall_score(ground_truth, pred)\n        f1 = f1_score(ground_truth, pred)\n\n        print(f\"Consumer: {consumer_id}\")\n        print(f\"Accuracy: {accuracy:.2f}\")\n        print(f\"Precision: {precision:.2f}\")\n        print(f\"Recall: {recall:.2f}\")\n        print(f\"F1 Score: {f1:.2f}\")\n        print(\"--------------\")\n\n        total_accuracy.append(accuracy)\n        total_precision.append(precision)\n        total_recall.append(recall)\n        total_f1.append(f1)\n\n        num_time_points = len(features)\n        num_features = features.shape[1]\n\n\n    print(f'Total accuracy: {sum(total_accuracy)/len(total_accuracy):.4f}', )\n    print(f'Total precision: {sum(total_precision)/len(total_precision):.4f}', )\n    print(f'Total recall: {sum(total_recall)/len(total_recall):.4f}', )\n    print(f'Total f1: {sum(total_f1)/len(total_f1):.4f}', )","block_group":"6410ef611f4d4c2eb2e27f77a4a9652d","execution_count":null,"outputs":[{"name":"stdout","text":"One Class SVM\n\n\nDataset: 1\nConsumer: 27396\nAccuracy: 0.62\nPrecision: 0.50\nRecall: 0.40\nF1 Score: 0.44\n--------------\nConsumer: 31709\nAccuracy: 0.58\nPrecision: 0.42\nRecall: 0.33\nF1 Score: 0.37\n--------------\nConsumer: 19373\nAccuracy: 0.55\nPrecision: 0.34\nRecall: 0.29\nF1 Score: 0.31\n--------------\nConsumer: 4691\nAccuracy: 0.65\nPrecision: 0.71\nRecall: 0.45\nF1 Score: 0.55\n--------------\nConsumer: 11612\nAccuracy: 0.48\nPrecision: 0.37\nRecall: 0.25\nF1 Score: 0.29\n--------------\nConsumer: 27966\nAccuracy: 0.68\nPrecision: 0.54\nRecall: 0.46\nF1 Score: 0.50\n--------------\nConsumer: 28310\nAccuracy: 0.59\nPrecision: 0.46\nRecall: 0.35\nF1 Score: 0.40\n--------------\nConsumer: 27391\nAccuracy: 0.44\nPrecision: 0.33\nRecall: 0.21\nF1 Score: 0.26\n--------------\nConsumer: 30199\nAccuracy: 0.41\nPrecision: 0.13\nRecall: 0.11\nF1 Score: 0.12\n--------------\nConsumer: 26952\nAccuracy: 0.54\nPrecision: 0.21\nRecall: 0.22\nF1 Score: 0.22\n--------------\nConsumer: 30894\nAccuracy: 0.55\nPrecision: 0.52\nRecall: 0.34\nF1 Score: 0.41\n--------------\nConsumer: 46170\nAccuracy: 0.56\nPrecision: 0.50\nRecall: 0.35\nF1 Score: 0.41\n--------------\nConsumer: 42635\nAccuracy: 0.54\nPrecision: 0.46\nRecall: 0.32\nF1 Score: 0.38\n--------------\nConsumer: 7020\nAccuracy: 0.83\nPrecision: 0.70\nRecall: 0.73\nF1 Score: 0.71\n--------------\nConsumer: 12791\nAccuracy: 0.55\nPrecision: 0.40\nRecall: 0.31\nF1 Score: 0.35\n--------------\nConsumer: 28961\nAccuracy: 0.70\nPrecision: 0.15\nRecall: 0.51\nF1 Score: 0.23\n--------------\nConsumer: 41436\nAccuracy: 0.78\nPrecision: 0.74\nRecall: 0.61\nF1 Score: 0.67\n--------------\nConsumer: 25414\nAccuracy: 0.47\nPrecision: 0.42\nRecall: 0.26\nF1 Score: 0.32\n--------------\nConsumer: 21496\nAccuracy: 0.77\nPrecision: 0.26\nRecall: 0.89\nF1 Score: 0.40\n--------------\nConsumer: 45806\nAccuracy: 0.57\nPrecision: 0.51\nRecall: 0.35\nF1 Score: 0.42\n--------------\nTotal accuracy: 0.5923\nTotal precision: 0.4325\nTotal recall: 0.3864\nTotal f1: 0.3873\nDataset: 2\nConsumer: 27396\nAccuracy: 0.77\nPrecision: 0.40\nRecall: 0.72\nF1 Score: 0.51\n--------------\nConsumer: 19373\nAccuracy: 0.80\nPrecision: 0.32\nRecall: 1.00\nF1 Score: 0.49\n--------------\nConsumer: 10042\nAccuracy: 0.73\nPrecision: 0.36\nRecall: 0.56\nF1 Score: 0.43\n--------------\nConsumer: 21532\nAccuracy: 0.72\nPrecision: 0.08\nRecall: 1.00\nF1 Score: 0.15\n--------------\nConsumer: 18786\nAccuracy: 0.79\nPrecision: 0.60\nRecall: 0.67\nF1 Score: 0.63\n--------------\nConsumer: 46170\nAccuracy: 0.67\nPrecision: 0.48\nRecall: 0.46\nF1 Score: 0.47\n--------------\nConsumer: 28639\nAccuracy: 0.64\nPrecision: 0.02\nRecall: 0.09\nF1 Score: 0.04\n--------------\nConsumer: 41383\nAccuracy: 0.53\nPrecision: 0.42\nRecall: 0.29\nF1 Score: 0.35\n--------------\nConsumer: 48747\nAccuracy: 0.68\nPrecision: 0.20\nRecall: 0.43\nF1 Score: 0.28\n--------------\nConsumer: 36438\nAccuracy: 0.59\nPrecision: 0.25\nRecall: 0.29\nF1 Score: 0.27\n--------------\nConsumer: 34715\nAccuracy: 0.68\nPrecision: 0.25\nRecall: 0.46\nF1 Score: 0.33\n--------------\nConsumer: 24746\nAccuracy: 0.63\nPrecision: 0.18\nRecall: 0.29\nF1 Score: 0.23\n--------------\nConsumer: 46526\nAccuracy: 0.47\nPrecision: 0.40\nRecall: 0.25\nF1 Score: 0.31\n--------------\nConsumer: 18245\nAccuracy: 0.71\nPrecision: 0.26\nRecall: 0.54\nF1 Score: 0.35\n--------------\nConsumer: 8355\nAccuracy: 0.42\nPrecision: 0.33\nRecall: 0.20\nF1 Score: 0.25\n--------------\nConsumer: 21496\nAccuracy: 0.54\nPrecision: 0.45\nRecall: 0.32\nF1 Score: 0.37\n--------------\nConsumer: 14105\nAccuracy: 0.79\nPrecision: 0.32\nRecall: 0.97\nF1 Score: 0.48\n--------------\nConsumer: 6465\nAccuracy: 0.68\nPrecision: 0.09\nRecall: 0.37\nF1 Score: 0.15\n--------------\nConsumer: 27095\nAccuracy: 0.70\nPrecision: 0.53\nRecall: 0.51\nF1 Score: 0.52\n--------------\nConsumer: 6770\nAccuracy: 0.72\nPrecision: 0.08\nRecall: 1.00\nF1 Score: 0.15\n--------------\nTotal accuracy: 0.6279\nTotal precision: 0.3677\nTotal recall: 0.4542\nTotal f1: 0.3630\nDataset: 3\nConsumer: 27396\nAccuracy: 0.58\nPrecision: 0.58\nRecall: 0.36\nF1 Score: 0.45\n--------------\nConsumer: 23361\nAccuracy: 0.72\nPrecision: 0.16\nRecall: 0.67\nF1 Score: 0.26\n--------------\nConsumer: 3832\nAccuracy: 0.70\nPrecision: 0.14\nRecall: 0.58\nF1 Score: 0.23\n--------------\nConsumer: 20985\nAccuracy: 0.58\nPrecision: 0.38\nRecall: 0.34\nF1 Score: 0.35\n--------------\nConsumer: 18786\nAccuracy: 0.76\nPrecision: 0.22\nRecall: 0.89\nF1 Score: 0.36\n--------------\nConsumer: 12636\nAccuracy: 0.46\nPrecision: 0.38\nRecall: 0.24\nF1 Score: 0.30\n--------------\nConsumer: 30894\nAccuracy: 0.76\nPrecision: 0.23\nRecall: 0.98\nF1 Score: 0.37\n--------------\nConsumer: 1318\nAccuracy: 0.67\nPrecision: 0.09\nRecall: 0.35\nF1 Score: 0.14\n--------------\nConsumer: 543\nAccuracy: 0.70\nPrecision: 0.60\nRecall: 0.50\nF1 Score: 0.54\n--------------\nConsumer: 4011\nAccuracy: 0.51\nPrecision: 0.29\nRecall: 0.24\nF1 Score: 0.26\n--------------\nConsumer: 38899\nAccuracy: 0.62\nPrecision: 0.43\nRecall: 0.38\nF1 Score: 0.41\n--------------\nConsumer: 7020\nAccuracy: 0.76\nPrecision: 0.33\nRecall: 0.74\nF1 Score: 0.46\n--------------\nConsumer: 8189\nAccuracy: 0.76\nPrecision: 0.23\nRecall: 0.93\nF1 Score: 0.37\n--------------\nConsumer: 20043\nAccuracy: 0.69\nPrecision: 0.43\nRecall: 0.47\nF1 Score: 0.45\n--------------\nConsumer: 27171\nAccuracy: 0.51\nPrecision: 0.23\nRecall: 0.21\nF1 Score: 0.22\n--------------\nConsumer: 27439\nAccuracy: 0.75\nPrecision: 0.19\nRecall: 0.80\nF1 Score: 0.31\n--------------\nConsumer: 21496\nAccuracy: 0.77\nPrecision: 0.24\nRecall: 0.98\nF1 Score: 0.38\n--------------\nConsumer: 28544\nAccuracy: 0.59\nPrecision: 0.37\nRecall: 0.33\nF1 Score: 0.35\n--------------\nConsumer: 7212\nAccuracy: 0.68\nPrecision: 0.42\nRecall: 0.46\nF1 Score: 0.44\n--------------\nConsumer: 10656\nAccuracy: 0.60\nPrecision: 0.07\nRecall: 0.16\nF1 Score: 0.10\n--------------\nTotal accuracy: 0.6382\nTotal precision: 0.3452\nTotal recall: 0.4796\nTotal f1: 0.3543\nDataset: 4\nConsumer: 25096\nAccuracy: 0.54\nPrecision: 0.45\nRecall: 0.31\nF1 Score: 0.36\n--------------\nConsumer: 19373\nAccuracy: 0.71\nPrecision: 0.03\nRecall: 1.00\nF1 Score: 0.05\n--------------\nConsumer: 23361\nAccuracy: 0.56\nPrecision: 0.53\nRecall: 0.35\nF1 Score: 0.42\n--------------\nConsumer: 29752\nAccuracy: 0.72\nPrecision: 0.09\nRecall: 0.91\nF1 Score: 0.17\n--------------\nConsumer: 27966\nAccuracy: 0.85\nPrecision: 0.53\nRecall: 0.97\nF1 Score: 0.68\n--------------\nConsumer: 1091\nAccuracy: 0.68\nPrecision: 0.10\nRecall: 0.34\nF1 Score: 0.16\n--------------\nConsumer: 18786\nAccuracy: 0.60\nPrecision: 0.55\nRecall: 0.39\nF1 Score: 0.46\n--------------\nConsumer: 42635\nAccuracy: 0.70\nPrecision: 0.14\nRecall: 0.47\nF1 Score: 0.22\n--------------\nConsumer: 543\nAccuracy: 0.66\nPrecision: 0.38\nRecall: 0.44\nF1 Score: 0.41\n--------------\nConsumer: 3799\nAccuracy: 0.64\nPrecision: 0.49\nRecall: 0.41\nF1 Score: 0.44\n--------------\nConsumer: 32895\nAccuracy: 0.50\nPrecision: 0.42\nRecall: 0.28\nF1 Score: 0.33\n--------------\nConsumer: 36438\nAccuracy: 0.73\nPrecision: 0.10\nRecall: 1.00\nF1 Score: 0.18\n--------------\nConsumer: 26637\nAccuracy: 0.71\nPrecision: 0.03\nRecall: 1.00\nF1 Score: 0.05\n--------------\nConsumer: 678\nAccuracy: 0.84\nPrecision: 0.48\nRecall: 0.98\nF1 Score: 0.65\n--------------\nConsumer: 31543\nAccuracy: 0.65\nPrecision: 0.51\nRecall: 0.42\nF1 Score: 0.46\n--------------\nConsumer: 1209\nAccuracy: 0.72\nPrecision: 0.47\nRecall: 0.53\nF1 Score: 0.50\n--------------\nConsumer: 18828\nAccuracy: 0.73\nPrecision: 0.66\nRecall: 0.53\nF1 Score: 0.59\n--------------\nConsumer: 45806\nAccuracy: 0.73\nPrecision: 0.30\nRecall: 0.60\nF1 Score: 0.40\n--------------\nConsumer: 10656\nAccuracy: 0.47\nPrecision: 0.23\nRecall: 0.19\nF1 Score: 0.21\n--------------\nConsumer: 2440\nAccuracy: 0.69\nPrecision: 0.26\nRecall: 0.48\nF1 Score: 0.34\n--------------\nTotal accuracy: 0.6464\nTotal precision: 0.3433\nTotal recall: 0.5047\nTotal f1: 0.3544\nDataset: 5\nConsumer: 19373\nAccuracy: 0.72\nPrecision: 0.34\nRecall: 0.55\nF1 Score: 0.42\n--------------\nConsumer: 31319\nAccuracy: 0.71\nPrecision: 0.33\nRecall: 0.53\nF1 Score: 0.41\n--------------\nConsumer: 10042\nAccuracy: 0.69\nPrecision: 0.16\nRecall: 0.41\nF1 Score: 0.23\n--------------\nConsumer: 38452\nAccuracy: 0.68\nPrecision: 0.59\nRecall: 0.48\nF1 Score: 0.53\n--------------\nConsumer: 26952\nAccuracy: 0.50\nPrecision: 0.18\nRecall: 0.17\nF1 Score: 0.18\n--------------\nConsumer: 1318\nAccuracy: 0.54\nPrecision: 0.51\nRecall: 0.32\nF1 Score: 0.40\n--------------\nConsumer: 32895\nAccuracy: 0.75\nPrecision: 0.16\nRecall: 1.00\nF1 Score: 0.28\n--------------\nConsumer: 7020\nAccuracy: 0.75\nPrecision: 0.16\nRecall: 1.00\nF1 Score: 0.27\n--------------\nConsumer: 49438\nAccuracy: 0.51\nPrecision: 0.29\nRecall: 0.24\nF1 Score: 0.26\n--------------\nConsumer: 678\nAccuracy: 0.82\nPrecision: 0.38\nRecall: 1.00\nF1 Score: 0.56\n--------------\nConsumer: 78\nAccuracy: 0.47\nPrecision: 0.25\nRecall: 0.20\nF1 Score: 0.22\n--------------\nConsumer: 5932\nAccuracy: 0.58\nPrecision: 0.33\nRecall: 0.31\nF1 Score: 0.32\n--------------\nConsumer: 48264\nAccuracy: 0.64\nPrecision: 0.22\nRecall: 0.35\nF1 Score: 0.27\n--------------\nConsumer: 27171\nAccuracy: 0.39\nPrecision: 0.26\nRecall: 0.17\nF1 Score: 0.20\n--------------\nConsumer: 2220\nAccuracy: 0.71\nPrecision: 0.55\nRecall: 0.52\nF1 Score: 0.53\n--------------\nConsumer: 21496\nAccuracy: 0.86\nPrecision: 0.79\nRecall: 0.75\nF1 Score: 0.77\n--------------\nConsumer: 28544\nAccuracy: 0.59\nPrecision: 0.45\nRecall: 0.36\nF1 Score: 0.40\n--------------\nConsumer: 14105\nAccuracy: 0.50\nPrecision: 0.41\nRecall: 0.27\nF1 Score: 0.33\n--------------\nConsumer: 43355\nAccuracy: 0.63\nPrecision: 0.21\nRecall: 0.32\nF1 Score: 0.25\n--------------\nConsumer: 45806\nAccuracy: 0.56\nPrecision: 0.50\nRecall: 0.34\nF1 Score: 0.41\n--------------\nTotal accuracy: 0.6430\nTotal precision: 0.3453\nTotal recall: 0.4968\nTotal f1: 0.3558\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":0},"cell_id":"131f0659fa25469f99e7746e636eb1e3","deepnote_cell_type":"text-cell-h2"},"source":"## SGD One Class SVM","block_group":"c36863128c7b45f89f05da7d0a12e847"},{"cell_type":"code","metadata":{"source_hash":"fd138a19","execution_start":1699248688509,"execution_millis":297525,"deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":0},"deepnote_to_be_reexecuted":false,"cell_id":"9324f04bb12b401eb95951973f322c61","deepnote_cell_type":"code"},"source":"total_accuracy = []\ntotal_precision = []\ntotal_recall = []\ntotal_f1 = []\nsgd_pred = []\n\nprint('SGD One Class SVM\\n\\n')\n\ncount = 0\nfor i in all_data:\n    count += 1\n    print(f'Dataset: {count}')\n    for j in range(len(i.index)):\n        consumer_data, consumer_id, fraud_point = consumerData(i, j)\n        fraud_point_week = (fraud_point/96)\n        weekly_consumption_year1, weekly_consumption_year2 = setupWeeklyData(consumer_data)\n\n        features = []\n        for week_data in weekly_consumption_year1 + weekly_consumption_year2:\n            result = catch22_all(week_data)\n            features.append(result['values'])\n\n        features = np.array(features)\n\n        # Check and handle NaN values \n        features = features[~np.isnan(features).any(axis=1)]\n\n        # Normalize the features\n        features = scaler.fit_transform(features)\n\n        # Applying PCA transformation\n        features = pca.fit_transform(features)\n        \n        sdgone = linear_model.SGDOneClassSVM(random_state=3, learning_rate='optimal')\n        sdgone.fit(features)\n\n        # Predict the anomalies\n        pred = sdgone.predict(features)\n\n        # Convert predictions to match ground truth format: 1 for anomalies and 0 for normal\n        pred[pred == 1] = 0\n        pred[pred == -1] = 1\n\n        # Get the ground truth\n        ground_truth = get_ground_truth(len(features), fraud_point_week)\n\n        # Calculate accuracy measures\n        pred = pred.tolist()\n        sgd_pred = pred\n        # print(\"Pred: \", pred, \"ground truth: \", ground_truth)\n\n        accuracy = accuracy_score(ground_truth, pred)\n        precision = precision_score(ground_truth, pred)\n        recall = recall_score(ground_truth, pred)\n        f1 = f1_score(ground_truth, pred)\n\n        print(f\"Consumer: {consumer_id}\")\n        print(f\"Accuracy: {accuracy:.2f}\")\n        print(f\"Precision: {precision:.2f}\")\n        print(f\"Recall: {recall:.2f}\")\n        print(f\"F1 Score: {f1:.2f}\")\n        print(\"--------------\")\n\n        total_accuracy.append(accuracy)\n        total_precision.append(precision)\n        total_recall.append(recall)\n        total_f1.append(f1)\n\n        num_time_points = len(features)\n        num_features = features.shape[1]\n\n\n    print(f'Total accuracy: {sum(total_accuracy)/len(total_accuracy):.4f}', )\n    print(f'Total precision: {sum(total_precision)/len(total_precision):.4f}', )\n    print(f'Total recall: {sum(total_recall)/len(total_recall):.4f}', )\n    print(f'Total f1: {sum(total_f1)/len(total_f1):.4f}', )","block_group":"3ee679aeac00423c88cd9af0367e2d09","execution_count":null,"outputs":[{"name":"stdout","text":"SGD One Class SVM\n\n\nDataset: 1\nConsumer: 27396\nAccuracy: 0.18\nPrecision: 0.15\nRecall: 0.25\nF1 Score: 0.18\n--------------\nConsumer: 31709\nAccuracy: 0.44\nPrecision: 0.33\nRecall: 0.50\nF1 Score: 0.40\n--------------\nConsumer: 19373\nAccuracy: 0.63\nPrecision: 0.48\nRecall: 0.72\nF1 Score: 0.57\n--------------\nConsumer: 4691\nAccuracy: 0.36\nPrecision: 0.35\nRecall: 0.40\nF1 Score: 0.37\n--------------\nConsumer: 11612\nAccuracy: 0.32\nPrecision: 0.31\nRecall: 0.45\nF1 Score: 0.37\n--------------\nConsumer: 27966\nAccuracy: 0.22\nPrecision: 0.13\nRecall: 0.23\nF1 Score: 0.17\n--------------\nConsumer: 28310\nAccuracy: 0.34\nPrecision: 0.24\nRecall: 0.31\nF1 Score: 0.27\n--------------\nConsumer: 27391\nAccuracy: 0.41\nPrecision: 0.36\nRecall: 0.36\nF1 Score: 0.36\n--------------\nConsumer: 30199\nAccuracy: 0.67\nPrecision: 0.53\nRecall: 0.84\nF1 Score: 0.65\n--------------\nConsumer: 26952\nAccuracy: 0.37\nPrecision: 0.15\nRecall: 0.26\nF1 Score: 0.19\n--------------\nConsumer: 30894\nAccuracy: 0.46\nPrecision: 0.42\nRecall: 0.41\nF1 Score: 0.42\n--------------\nConsumer: 46170\nAccuracy: 0.35\nPrecision: 0.29\nRecall: 0.35\nF1 Score: 0.32\n--------------\nConsumer: 42635\nAccuracy: 0.44\nPrecision: 0.37\nRecall: 0.42\nF1 Score: 0.39\n--------------\nConsumer: 7020\nAccuracy: 0.22\nPrecision: 0.12\nRecall: 0.26\nF1 Score: 0.16\n--------------\nConsumer: 12791\nAccuracy: 0.51\nPrecision: 0.36\nRecall: 0.33\nF1 Score: 0.34\n--------------\nConsumer: 28961\nAccuracy: 0.58\nPrecision: 0.14\nRecall: 0.75\nF1 Score: 0.24\n--------------\nConsumer: 41436\nAccuracy: 0.55\nPrecision: 0.41\nRecall: 0.51\nF1 Score: 0.45\n--------------\nConsumer: 25414\nAccuracy: 0.17\nPrecision: 0.15\nRecall: 0.16\nF1 Score: 0.16\n--------------\nConsumer: 21496\nAccuracy: 0.48\nPrecision: 0.08\nRecall: 0.48\nF1 Score: 0.14\n--------------\nConsumer: 45806\nAccuracy: 0.42\nPrecision: 0.28\nRecall: 0.21\nF1 Score: 0.24\n--------------\nTotal accuracy: 0.4054\nTotal precision: 0.2828\nTotal recall: 0.4092\nTotal f1: 0.3198\nDataset: 2\nConsumer: 27396\nAccuracy: 0.39\nPrecision: 0.15\nRecall: 0.56\nF1 Score: 0.24\n--------------\nConsumer: 19373\nAccuracy: 0.35\nPrecision: 0.04\nRecall: 0.27\nF1 Score: 0.08\n--------------\nConsumer: 10042\nAccuracy: 0.43\nPrecision: 0.13\nRecall: 0.36\nF1 Score: 0.19\n--------------\nConsumer: 21532\nAccuracy: 0.59\nPrecision: 0.02\nRecall: 0.28\nF1 Score: 0.03\n--------------\nConsumer: 18786\nAccuracy: 0.29\nPrecision: 0.06\nRecall: 0.12\nF1 Score: 0.08\n--------------\nConsumer: 46170\nAccuracy: 0.44\nPrecision: 0.27\nRecall: 0.46\nF1 Score: 0.35\n--------------\nConsumer: 28639\nAccuracy: 0.62\nPrecision: 0.14\nRecall: 0.76\nF1 Score: 0.23\n--------------\nConsumer: 41383\nAccuracy: 0.21\nPrecision: 0.11\nRecall: 0.12\nF1 Score: 0.11\n--------------\nConsumer: 48747\nAccuracy: 0.55\nPrecision: 0.19\nRecall: 0.66\nF1 Score: 0.30\n--------------\nConsumer: 36438\nAccuracy: 0.43\nPrecision: 0.00\nRecall: 0.01\nF1 Score: 0.00\n--------------\nConsumer: 34715\nAccuracy: 0.36\nPrecision: 0.00\nRecall: 0.01\nF1 Score: 0.00\n--------------\nConsumer: 24746\nAccuracy: 0.57\nPrecision: 0.29\nRecall: 0.93\nF1 Score: 0.45\n--------------\nConsumer: 46526\nAccuracy: 0.52\nPrecision: 0.49\nRecall: 0.49\nF1 Score: 0.49\n--------------\nConsumer: 18245\nAccuracy: 0.48\nPrecision: 0.04\nRecall: 0.13\nF1 Score: 0.07\n--------------\nConsumer: 8355\nAccuracy: 0.51\nPrecision: 0.49\nRecall: 0.39\nF1 Score: 0.43\n--------------\nConsumer: 21496\nAccuracy: 0.47\nPrecision: 0.38\nRecall: 0.34\nF1 Score: 0.36\n--------------\nConsumer: 14105\nAccuracy: 0.33\nPrecision: 0.00\nRecall: 0.00\nF1 Score: 0.00\n--------------\nConsumer: 6465\nAccuracy: 0.40\nPrecision: 0.05\nRecall: 0.39\nF1 Score: 0.09\n--------------\nConsumer: 27095\nAccuracy: 0.29\nPrecision: 0.17\nRecall: 0.31\nF1 Score: 0.22\n--------------\nConsumer: 6770\nAccuracy: 0.47\nPrecision: 0.00\nRecall: 0.06\nF1 Score: 0.01\n--------------\nTotal accuracy: 0.4207\nTotal precision: 0.2173\nTotal recall: 0.3706\nTotal f1: 0.2529\nDataset: 3\nConsumer: 27396\nAccuracy: 0.71\nPrecision: 0.77\nRecall: 0.56\nF1 Score: 0.64\n--------------\nConsumer: 23361\nAccuracy: 0.66\nPrecision: 0.18\nRecall: 1.00\nF1 Score: 0.30\n--------------\nConsumer: 3832\nAccuracy: 0.49\nPrecision: 0.03\nRecall: 0.20\nF1 Score: 0.06\n--------------\nConsumer: 20985\nAccuracy: 0.45\nPrecision: 0.24\nRecall: 0.29\nF1 Score: 0.26\n--------------\nConsumer: 18786\nAccuracy: 0.47\nPrecision: 0.05\nRecall: 0.33\nF1 Score: 0.09\n--------------\nConsumer: 12636\nAccuracy: 0.06\nPrecision: 0.05\nRecall: 0.05\nF1 Score: 0.05\n--------------\nConsumer: 30894\nAccuracy: 0.68\nPrecision: 0.09\nRecall: 0.40\nF1 Score: 0.15\n--------------\nConsumer: 1318\nAccuracy: 0.53\nPrecision: 0.08\nRecall: 0.49\nF1 Score: 0.14\n--------------\nConsumer: 543\nAccuracy: 0.53\nPrecision: 0.39\nRecall: 0.53\nF1 Score: 0.45\n--------------\nConsumer: 4011\nAccuracy: 0.19\nPrecision: 0.07\nRecall: 0.10\nF1 Score: 0.08\n--------------\nConsumer: 38899\nAccuracy: 0.39\nPrecision: 0.23\nRecall: 0.34\nF1 Score: 0.27\n--------------\nConsumer: 7020\nAccuracy: 0.44\nPrecision: 0.11\nRecall: 0.44\nF1 Score: 0.17\n--------------\nConsumer: 8189\nAccuracy: 0.45\nPrecision: 0.06\nRecall: 0.39\nF1 Score: 0.10\n--------------\nConsumer: 20043\nAccuracy: 0.31\nPrecision: 0.05\nRecall: 0.08\nF1 Score: 0.06\n--------------\nConsumer: 27171\nAccuracy: 0.63\nPrecision: 0.46\nRecall: 0.75\nF1 Score: 0.57\n--------------\nConsumer: 27439\nAccuracy: 0.38\nPrecision: 0.00\nRecall: 0.00\nF1 Score: 0.00\n--------------\nConsumer: 21496\nAccuracy: 0.41\nPrecision: 0.06\nRecall: 0.44\nF1 Score: 0.10\n--------------\nConsumer: 28544\nAccuracy: 0.55\nPrecision: 0.38\nRecall: 0.54\nF1 Score: 0.44\n--------------\nConsumer: 7212\nAccuracy: 0.55\nPrecision: 0.26\nRecall: 0.36\nF1 Score: 0.30\n--------------\nConsumer: 10656\nAccuracy: 0.33\nPrecision: 0.05\nRecall: 0.23\nF1 Score: 0.08\n--------------\nTotal accuracy: 0.4340\nTotal precision: 0.2047\nTotal recall: 0.3724\nTotal f1: 0.2406\nDataset: 4\nConsumer: 25096\nAccuracy: 0.87\nPrecision: 0.80\nRecall: 0.93\nF1 Score: 0.86\n--------------\nConsumer: 19373\nAccuracy: 0.46\nPrecision: 0.00\nRecall: 0.00\nF1 Score: 0.00\n--------------\nConsumer: 23361\nAccuracy: 0.84\nPrecision: 0.81\nRecall: 0.83\nF1 Score: 0.82\n--------------\nConsumer: 29752\nAccuracy: 0.39\nPrecision: 0.02\nRecall: 0.36\nF1 Score: 0.04\n--------------\nConsumer: 27966\nAccuracy: 0.13\nPrecision: 0.00\nRecall: 0.00\nF1 Score: 0.00\n--------------\nConsumer: 1091\nAccuracy: 0.35\nPrecision: 0.01\nRecall: 0.06\nF1 Score: 0.02\n--------------\nConsumer: 18786\nAccuracy: 0.61\nPrecision: 0.53\nRecall: 0.70\nF1 Score: 0.61\n--------------\nConsumer: 42635\nAccuracy: 0.38\nPrecision: 0.01\nRecall: 0.05\nF1 Score: 0.01\n--------------\nConsumer: 543\nAccuracy: 0.79\nPrecision: 0.58\nRecall: 0.78\nF1 Score: 0.67\n--------------\nConsumer: 3799\nAccuracy: 0.37\nPrecision: 0.21\nRecall: 0.27\nF1 Score: 0.23\n--------------\nConsumer: 32895\nAccuracy: 0.24\nPrecision: 0.14\nRecall: 0.13\nF1 Score: 0.13\n--------------\nConsumer: 36438\nAccuracy: 0.44\nPrecision: 0.01\nRecall: 0.18\nF1 Score: 0.02\n--------------\nConsumer: 26637\nAccuracy: 0.32\nPrecision: 0.00\nRecall: 0.00\nF1 Score: 0.00\n--------------\nConsumer: 678\nAccuracy: 0.29\nPrecision: 0.01\nRecall: 0.05\nF1 Score: 0.02\n--------------\nConsumer: 31543\nAccuracy: 0.36\nPrecision: 0.16\nRecall: 0.19\nF1 Score: 0.18\n--------------\nConsumer: 1209\nAccuracy: 0.47\nPrecision: 0.24\nRecall: 0.44\nF1 Score: 0.31\n--------------\nConsumer: 18828\nAccuracy: 0.52\nPrecision: 0.39\nRecall: 0.53\nF1 Score: 0.45\n--------------\nConsumer: 45806\nAccuracy: 0.06\nPrecision: 0.00\nRecall: 0.00\nF1 Score: 0.00\n--------------\nConsumer: 10656\nAccuracy: 0.59\nPrecision: 0.46\nRecall: 0.66\nF1 Score: 0.54\n--------------\nConsumer: 2440\nAccuracy: 0.34\nPrecision: 0.01\nRecall: 0.04\nF1 Score: 0.02\n--------------\nTotal accuracy: 0.4357\nTotal precision: 0.2083\nTotal recall: 0.3569\nTotal f1: 0.2420\nDataset: 5\nConsumer: 19373\nAccuracy: 0.23\nPrecision: 0.04\nRecall: 0.13\nF1 Score: 0.06\n--------------\nConsumer: 31319\nAccuracy: 0.47\nPrecision: 0.21\nRecall: 0.68\nF1 Score: 0.32\n--------------\nConsumer: 10042\nAccuracy: 0.33\nPrecision: 0.00\nRecall: 0.00\nF1 Score: 0.00\n--------------\nConsumer: 38452\nAccuracy: 0.27\nPrecision: 0.13\nRecall: 0.17\nF1 Score: 0.15\n--------------\nConsumer: 26952\nAccuracy: 0.41\nPrecision: 0.17\nRecall: 0.23\nF1 Score: 0.20\n--------------\nConsumer: 1318\nAccuracy: 0.70\nPrecision: 0.74\nRecall: 0.56\nF1 Score: 0.63\n--------------\nConsumer: 32895\nAccuracy: 0.42\nPrecision: 0.00\nRecall: 0.03\nF1 Score: 0.00\n--------------\nConsumer: 7020\nAccuracy: 0.60\nPrecision: 0.11\nRecall: 1.00\nF1 Score: 0.19\n--------------\nConsumer: 49438\nAccuracy: 0.46\nPrecision: 0.33\nRecall: 0.45\nF1 Score: 0.38\n--------------\nConsumer: 678\nAccuracy: 0.02\nPrecision: 0.00\nRecall: 0.00\nF1 Score: 0.00\n--------------\nConsumer: 78\nAccuracy: 0.44\nPrecision: 0.31\nRecall: 0.40\nF1 Score: 0.35\n--------------\nConsumer: 5932\nAccuracy: 0.17\nPrecision: 0.03\nRecall: 0.04\nF1 Score: 0.03\n--------------\nConsumer: 48264\nAccuracy: 0.58\nPrecision: 0.27\nRecall: 0.69\nF1 Score: 0.39\n--------------\nConsumer: 27171\nAccuracy: 0.79\nPrecision: 0.72\nRecall: 0.92\nF1 Score: 0.81\n--------------\nConsumer: 2220\nAccuracy: 0.56\nPrecision: 0.39\nRecall: 0.65\nF1 Score: 0.48\n--------------\nConsumer: 21496\nAccuracy: 0.63\nPrecision: 0.43\nRecall: 0.63\nF1 Score: 0.51\n--------------\nConsumer: 28544\nAccuracy: 0.50\nPrecision: 0.38\nRecall: 0.51\nF1 Score: 0.43\n--------------\nConsumer: 14105\nAccuracy: 0.08\nPrecision: 0.01\nRecall: 0.02\nF1 Score: 0.02\n--------------\nConsumer: 43355\nAccuracy: 0.52\nPrecision: 0.11\nRecall: 0.21\nF1 Score: 0.15\n--------------\nConsumer: 45806\nAccuracy: 0.43\nPrecision: 0.38\nRecall: 0.41\nF1 Score: 0.39\n--------------\nTotal accuracy: 0.4347\nTotal precision: 0.2141\nTotal recall: 0.3628\nTotal f1: 0.2485\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":0},"cell_id":"e9fe0424b572462796512b6c0cf6fda4","deepnote_cell_type":"text-cell-h2"},"source":"## Local Outlier Factor","block_group":"c8468399cd8444c89f8cb7df73f79f9f"},{"cell_type":"code","metadata":{"source_hash":"443f8a89","execution_start":1699248986116,"execution_millis":307638,"deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":0},"deepnote_to_be_reexecuted":false,"cell_id":"884df6c6553a43619cd01bfc184963b6","deepnote_cell_type":"code"},"source":"total_accuracy = []\ntotal_precision = []\ntotal_recall = []\ntotal_f1 = []\nlof_pred = []\n\nprint('Local Outlier Factor\\n\\n')\n\ncount = 0\nfor i in all_data:\n    count += 1\n    print(f'Dataset: {count}')\n    for j in range(len(i.index)):\n        consumer_data, consumer_id, fraud_point = consumerData(i, j)\n        fraud_point_week = (fraud_point/96)\n        weekly_consumption_year1, weekly_consumption_year2 = setupWeeklyData(consumer_data)\n\n        features = []\n        for week_data in weekly_consumption_year1 + weekly_consumption_year2:\n            result = catch22_all(week_data)\n            features.append(result['values'])\n\n        features = np.array(features)\n\n        # Check and handle NaN values \n        features = features[~np.isnan(features).any(axis=1)]\n\n        # Normalize the features\n        features = scaler.fit_transform(features)\n\n        # Applying PCA transformation\n        features = pca.fit_transform(features)\n        \n        lof = LocalOutlierFactor(n_neighbors=50, algorithm='auto', contamination=float(0.1), novelty=True)\n        lof.fit(features)\n        pred = lof.predict(features)\n\n        # Convert predictions to match ground truth format: 1 for anomalies and 0 for normal\n        pred[pred == 1] = 0\n        pred[pred == -1] = 1\n\n        # Get the ground truth\n        ground_truth = get_ground_truth(len(features), fraud_point_week)\n\n        # Calculate accuracy measures\n        pred = pred.tolist()\n        lof_pred = pred\n        # print(\"Pred: \", pred, \"ground truth: \", ground_truth)\n\n        accuracy = accuracy_score(ground_truth, pred)\n        precision = precision_score(ground_truth, pred)\n        recall = recall_score(ground_truth, pred)\n        f1 = f1_score(ground_truth, pred)\n\n        print(f\"Consumer: {consumer_id}\")\n        print(f\"Accuracy: {accuracy:.2f}\")\n        print(f\"Precision: {precision:.2f}\")\n        print(f\"Recall: {recall:.2f}\")\n        print(f\"F1 Score: {f1:.2f}\")\n        print(\"--------------\")\n\n        total_accuracy.append(accuracy)\n        total_precision.append(precision)\n        total_recall.append(recall)\n        total_f1.append(f1)\n\n        num_time_points = len(features)\n        num_features = features.shape[1]\n\n\n    print(f'Total accuracy: {sum(total_accuracy)/len(total_accuracy):.4f}', )\n    print(f'Total precision: {sum(total_precision)/len(total_precision):.4f}', )\n    print(f'Total recall: {sum(total_recall)/len(total_recall):.4f}', )\n    print(f'Total f1: {sum(total_f1)/len(total_f1):.4f}', )","block_group":"9a6729acadd14f62b95f3e2be4c61726","execution_count":null,"outputs":[{"name":"stdout","text":"Local Outlier Factor\n\n\nDataset: 1\nConsumer: 27396\nAccuracy: 0.59\nPrecision: 0.28\nRecall: 0.07\nF1 Score: 0.11\n--------------\nConsumer: 31709\nAccuracy: 0.60\nPrecision: 0.36\nRecall: 0.09\nF1 Score: 0.15\n--------------\nConsumer: 19373\nAccuracy: 0.61\nPrecision: 0.30\nRecall: 0.08\nF1 Score: 0.13\n--------------\nConsumer: 4691\nAccuracy: 0.51\nPrecision: 0.46\nRecall: 0.09\nF1 Score: 0.16\n--------------\nConsumer: 11612\nAccuracy: 0.56\nPrecision: 0.52\nRecall: 0.12\nF1 Score: 0.19\n--------------\nConsumer: 27966\nAccuracy: 0.61\nPrecision: 0.28\nRecall: 0.08\nF1 Score: 0.12\n--------------\nConsumer: 28310\nAccuracy: 0.59\nPrecision: 0.39\nRecall: 0.09\nF1 Score: 0.15\n--------------\nConsumer: 27391\nAccuracy: 0.50\nPrecision: 0.29\nRecall: 0.06\nF1 Score: 0.10\n--------------\nConsumer: 30199\nAccuracy: 0.62\nPrecision: 0.38\nRecall: 0.10\nF1 Score: 0.15\n--------------\nConsumer: 26952\nAccuracy: 0.63\nPrecision: 0.06\nRecall: 0.02\nF1 Score: 0.03\n--------------\nConsumer: 30894\nAccuracy: 0.50\nPrecision: 0.34\nRecall: 0.07\nF1 Score: 0.12\n--------------\nConsumer: 46170\nAccuracy: 0.56\nPrecision: 0.51\nRecall: 0.11\nF1 Score: 0.19\n--------------\nConsumer: 42635\nAccuracy: 0.49\nPrecision: 0.14\nRecall: 0.03\nF1 Score: 0.05\n--------------\nConsumer: 7020\nAccuracy: 0.73\nPrecision: 0.59\nRecall: 0.20\nF1 Score: 0.30\n--------------\nConsumer: 12791\nAccuracy: 0.56\nPrecision: 0.21\nRecall: 0.05\nF1 Score: 0.08\n--------------\nConsumer: 28961\nAccuracy: 0.84\nPrecision: 0.12\nRecall: 0.13\nF1 Score: 0.12\n--------------\nConsumer: 41436\nAccuracy: 0.69\nPrecision: 0.76\nRecall: 0.20\nF1 Score: 0.32\n--------------\nConsumer: 25414\nAccuracy: 0.53\nPrecision: 0.55\nRecall: 0.11\nF1 Score: 0.18\n--------------\nConsumer: 21496\nAccuracy: 0.85\nPrecision: 0.17\nRecall: 0.19\nF1 Score: 0.18\n--------------\nConsumer: 45806\nAccuracy: 0.57\nPrecision: 0.56\nRecall: 0.13\nF1 Score: 0.21\n--------------\nTotal accuracy: 0.6070\nTotal precision: 0.3638\nTotal recall: 0.1016\nTotal f1: 0.1524\nDataset: 2\nConsumer: 27396\nAccuracy: 0.77\nPrecision: 0.17\nRecall: 0.10\nF1 Score: 0.13\n--------------\nConsumer: 19373\nAccuracy: 0.85\nPrecision: 0.22\nRecall: 0.21\nF1 Score: 0.22\n--------------\nConsumer: 10042\nAccuracy: 0.77\nPrecision: 0.29\nRecall: 0.15\nF1 Score: 0.20\n--------------\nConsumer: 21532\nAccuracy: 0.92\nPrecision: 0.22\nRecall: 0.83\nF1 Score: 0.34\n--------------\nConsumer: 18786\nAccuracy: 0.75\nPrecision: 0.59\nRecall: 0.22\nF1 Score: 0.32\n--------------\nConsumer: 46170\nAccuracy: 0.67\nPrecision: 0.41\nRecall: 0.12\nF1 Score: 0.19\n--------------\nConsumer: 28639\nAccuracy: 0.83\nPrecision: 0.01\nRecall: 0.02\nF1 Score: 0.02\n--------------\nConsumer: 41383\nAccuracy: 0.56\nPrecision: 0.46\nRecall: 0.10\nF1 Score: 0.17\n--------------\nConsumer: 48747\nAccuracy: 0.79\nPrecision: 0.17\nRecall: 0.12\nF1 Score: 0.14\n--------------\nConsumer: 36438\nAccuracy: 0.67\nPrecision: 0.15\nRecall: 0.06\nF1 Score: 0.08\n--------------\nConsumer: 34715\nAccuracy: 0.75\nPrecision: 0.06\nRecall: 0.03\nF1 Score: 0.04\n--------------\nConsumer: 24746\nAccuracy: 0.74\nPrecision: 0.12\nRecall: 0.06\nF1 Score: 0.08\n--------------\nConsumer: 46526\nAccuracy: 0.50\nPrecision: 0.36\nRecall: 0.08\nF1 Score: 0.12\n--------------\nConsumer: 18245\nAccuracy: 0.86\nPrecision: 0.51\nRecall: 0.34\nF1 Score: 0.41\n--------------\nConsumer: 8355\nAccuracy: 0.49\nPrecision: 0.32\nRecall: 0.06\nF1 Score: 0.11\n--------------\nConsumer: 21496\nAccuracy: 0.57\nPrecision: 0.49\nRecall: 0.11\nF1 Score: 0.19\n--------------\nConsumer: 14105\nAccuracy: 0.82\nPrecision: 0.06\nRecall: 0.06\nF1 Score: 0.06\n--------------\nConsumer: 6465\nAccuracy: 0.86\nPrecision: 0.19\nRecall: 0.24\nF1 Score: 0.21\n--------------\nConsumer: 27095\nAccuracy: 0.60\nPrecision: 0.05\nRecall: 0.01\nF1 Score: 0.02\n--------------\nConsumer: 6770\nAccuracy: 0.93\nPrecision: 0.26\nRecall: 1.00\nF1 Score: 0.42\n--------------\nTotal accuracy: 0.6710\nTotal precision: 0.3097\nTotal recall: 0.1491\nTotal f1: 0.1626\nDataset: 3\nConsumer: 27396\nAccuracy: 0.46\nPrecision: 0.09\nRecall: 0.02\nF1 Score: 0.03\n--------------\nConsumer: 23361\nAccuracy: 0.85\nPrecision: 0.12\nRecall: 0.15\nF1 Score: 0.13\n--------------\nConsumer: 3832\nAccuracy: 0.84\nPrecision: 0.09\nRecall: 0.11\nF1 Score: 0.10\n--------------\nConsumer: 20985\nAccuracy: 0.60\nPrecision: 0.22\nRecall: 0.07\nF1 Score: 0.10\n--------------\nConsumer: 18786\nAccuracy: 0.95\nPrecision: 0.61\nRecall: 0.80\nF1 Score: 0.69\n--------------\nConsumer: 12636\nAccuracy: 0.52\nPrecision: 0.45\nRecall: 0.09\nF1 Score: 0.15\n--------------\nConsumer: 30894\nAccuracy: 0.84\nPrecision: 0.01\nRecall: 0.02\nF1 Score: 0.02\n--------------\nConsumer: 1318\nAccuracy: 0.84\nPrecision: 0.06\nRecall: 0.07\nF1 Score: 0.06\n--------------\nConsumer: 543\nAccuracy: 0.65\nPrecision: 0.57\nRecall: 0.15\nF1 Score: 0.23\n--------------\nConsumer: 4011\nAccuracy: 0.56\nPrecision: 0.08\nRecall: 0.02\nF1 Score: 0.03\n--------------\nConsumer: 38899\nAccuracy: 0.65\nPrecision: 0.43\nRecall: 0.12\nF1 Score: 0.19\n--------------\nConsumer: 7020\nAccuracy: 0.88\nPrecision: 0.59\nRecall: 0.43\nF1 Score: 0.49\n--------------\nConsumer: 8189\nAccuracy: 0.85\nPrecision: 0.10\nRecall: 0.13\nF1 Score: 0.11\n--------------\nConsumer: 20043\nAccuracy: 0.66\nPrecision: 0.15\nRecall: 0.05\nF1 Score: 0.08\n--------------\nConsumer: 27171\nAccuracy: 0.61\nPrecision: 0.19\nRecall: 0.05\nF1 Score: 0.09\n--------------\nConsumer: 27439\nAccuracy: 0.85\nPrecision: 0.06\nRecall: 0.08\nF1 Score: 0.07\n--------------\nConsumer: 21496\nAccuracy: 0.85\nPrecision: 0.13\nRecall: 0.17\nF1 Score: 0.15\n--------------\nConsumer: 28544\nAccuracy: 0.68\nPrecision: 0.56\nRecall: 0.17\nF1 Score: 0.26\n--------------\nConsumer: 7212\nAccuracy: 0.66\nPrecision: 0.15\nRecall: 0.05\nF1 Score: 0.08\n--------------\nConsumer: 10656\nAccuracy: 0.78\nPrecision: 0.02\nRecall: 0.01\nF1 Score: 0.01\n--------------\nTotal accuracy: 0.6903\nTotal precision: 0.2841\nTotal recall: 0.1454\nTotal f1: 0.1594\nDataset: 4\nConsumer: 25096\nAccuracy: 0.54\nPrecision: 0.33\nRecall: 0.07\nF1 Score: 0.12\n--------------\nConsumer: 19373\nAccuracy: 0.91\nPrecision: 0.09\nRecall: 1.00\nF1 Score: 0.16\n--------------\nConsumer: 23361\nAccuracy: 0.53\nPrecision: 0.37\nRecall: 0.08\nF1 Score: 0.13\n--------------\nConsumer: 29752\nAccuracy: 0.90\nPrecision: 0.13\nRecall: 0.41\nF1 Score: 0.20\n--------------\nConsumer: 27966\nAccuracy: 0.78\nPrecision: 0.18\nRecall: 0.10\nF1 Score: 0.13\n--------------\nConsumer: 1091\nAccuracy: 0.83\nPrecision: 0.10\nRecall: 0.11\nF1 Score: 0.11\n--------------\nConsumer: 18786\nAccuracy: 0.58\nPrecision: 0.56\nRecall: 0.12\nF1 Score: 0.20\n--------------\nConsumer: 42635\nAccuracy: 0.83\nPrecision: 0.07\nRecall: 0.08\nF1 Score: 0.07\n--------------\nConsumer: 543\nAccuracy: 0.67\nPrecision: 0.18\nRecall: 0.06\nF1 Score: 0.09\n--------------\nConsumer: 3799\nAccuracy: 0.60\nPrecision: 0.28\nRecall: 0.07\nF1 Score: 0.12\n--------------\nConsumer: 32895\nAccuracy: 0.54\nPrecision: 0.44\nRecall: 0.09\nF1 Score: 0.15\n--------------\nConsumer: 36438\nAccuracy: 0.94\nPrecision: 0.33\nRecall: 1.00\nF1 Score: 0.49\n--------------\nConsumer: 26637\nAccuracy: 0.91\nPrecision: 0.09\nRecall: 1.00\nF1 Score: 0.16\n--------------\nConsumer: 678\nAccuracy: 0.77\nPrecision: 0.08\nRecall: 0.05\nF1 Score: 0.06\n--------------\nConsumer: 31543\nAccuracy: 0.61\nPrecision: 0.34\nRecall: 0.09\nF1 Score: 0.14\n--------------\nConsumer: 1209\nAccuracy: 0.79\nPrecision: 0.77\nRecall: 0.28\nF1 Score: 0.42\n--------------\nConsumer: 18828\nAccuracy: 0.71\nPrecision: 0.88\nRecall: 0.23\nF1 Score: 0.36\n--------------\nConsumer: 45806\nAccuracy: 0.77\nPrecision: 0.10\nRecall: 0.07\nF1 Score: 0.08\n--------------\nConsumer: 10656\nAccuracy: 0.60\nPrecision: 0.32\nRecall: 0.08\nF1 Score: 0.13\n--------------\nConsumer: 2440\nAccuracy: 0.75\nPrecision: 0.04\nRecall: 0.03\nF1 Score: 0.03\n--------------\nTotal accuracy: 0.6996\nTotal precision: 0.2840\nTotal recall: 0.1718\nTotal f1: 0.1616\nDataset: 5\nConsumer: 19373\nAccuracy: 0.76\nPrecision: 0.23\nRecall: 0.12\nF1 Score: 0.16\n--------------\nConsumer: 31319\nAccuracy: 0.80\nPrecision: 0.44\nRecall: 0.23\nF1 Score: 0.30\n--------------\nConsumer: 10042\nAccuracy: 0.80\nPrecision: 0.09\nRecall: 0.07\nF1 Score: 0.08\n--------------\nConsumer: 38452\nAccuracy: 0.64\nPrecision: 0.55\nRecall: 0.14\nF1 Score: 0.22\n--------------\nConsumer: 26952\nAccuracy: 0.64\nPrecision: 0.21\nRecall: 0.06\nF1 Score: 0.10\n--------------\nConsumer: 1318\nAccuracy: 0.57\nPrecision: 0.67\nRecall: 0.14\nF1 Score: 0.23\n--------------\nConsumer: 32895\nAccuracy: 0.87\nPrecision: 0.03\nRecall: 0.06\nF1 Score: 0.04\n--------------\nConsumer: 7020\nAccuracy: 0.93\nPrecision: 0.37\nRecall: 0.68\nF1 Score: 0.47\n--------------\nConsumer: 49438\nAccuracy: 0.56\nPrecision: 0.11\nRecall: 0.03\nF1 Score: 0.05\n--------------\nConsumer: 678\nAccuracy: 0.82\nPrecision: 0.14\nRecall: 0.12\nF1 Score: 0.13\n--------------\nConsumer: 78\nAccuracy: 0.58\nPrecision: 0.26\nRecall: 0.07\nF1 Score: 0.11\n--------------\nConsumer: 5932\nAccuracy: 0.63\nPrecision: 0.21\nRecall: 0.07\nF1 Score: 0.10\n--------------\nConsumer: 48264\nAccuracy: 0.75\nPrecision: 0.21\nRecall: 0.10\nF1 Score: 0.14\n--------------\nConsumer: 27171\nAccuracy: 0.45\nPrecision: 0.03\nRecall: 0.01\nF1 Score: 0.01\n--------------\nConsumer: 2220\nAccuracy: 0.73\nPrecision: 0.76\nRecall: 0.23\nF1 Score: 0.36\n--------------\nConsumer: 21496\nAccuracy: 0.74\nPrecision: 0.77\nRecall: 0.23\nF1 Score: 0.35\n--------------\nConsumer: 28544\nAccuracy: 0.62\nPrecision: 0.51\nRecall: 0.13\nF1 Score: 0.21\n--------------\nConsumer: 14105\nAccuracy: 0.51\nPrecision: 0.29\nRecall: 0.06\nF1 Score: 0.10\n--------------\nConsumer: 43355\nAccuracy: 0.73\nPrecision: 0.13\nRecall: 0.06\nF1 Score: 0.09\n--------------\nConsumer: 45806\nAccuracy: 0.55\nPrecision: 0.47\nRecall: 0.10\nF1 Score: 0.16\n--------------\nTotal accuracy: 0.6964\nTotal precision: 0.2920\nTotal recall: 0.1646\nTotal f1: 0.1633\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":0},"cell_id":"cd71e60877794471844121d3385f9b87","deepnote_cell_type":"text-cell-h2"},"source":"## KMeans","block_group":"a156254e45ce470ea933c674088f7dfe"},{"cell_type":"code","metadata":{"source_hash":"7f23e866","execution_start":1699249293811,"execution_millis":317034,"deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":0},"deepnote_to_be_reexecuted":false,"cell_id":"12462ab528024517be722893c28112ff","deepnote_cell_type":"code"},"source":"total_accuracy = []\ntotal_precision = []\ntotal_recall = []\ntotal_f1 = []\nkmeans_pred = []\n\nprint('KMeans\\n\\n')\n\ncount = 0\nfor i in all_data:\n    count += 1\n    print(f'Dataset: {count}')\n    for j in range(len(i.index)):\n        consumer_data, consumer_id, fraud_point = consumerData(i, j)\n        fraud_point_week = (fraud_point/96)\n        weekly_consumption_year1, weekly_consumption_year2 = setupWeeklyData(consumer_data)\n\n        features = []\n        for week_data in weekly_consumption_year1 + weekly_consumption_year2:\n            result = catch22_all(week_data)\n            features.append(result['values'])\n\n        features = np.array(features)\n        features = features[~np.isnan(features).any(axis=1)]\n        features = scaler.fit_transform(features)\n        features = pca.fit_transform(features)\n\n        # Using KMeans clustering\n        kmeans = KMeans(n_clusters=2, n_init=10)  # using 2 clusters; normal and anomaly\n        kmeans.fit(features)\n        distances = np.linalg.norm(features - kmeans.cluster_centers_[kmeans.labels_], axis=1)\n        anomaly_threshold = np.percentile(distances, 90)\n        pred = np.where(distances > anomaly_threshold, 1, 0)  # 1 for anomalies and 0 for normal\n        kmeans_pred = pred\n\n        ground_truth = get_ground_truth(len(features), fraud_point_week)\n\n        accuracy = accuracy_score(ground_truth, pred)\n        precision = precision_score(ground_truth, pred)\n        recall = recall_score(ground_truth, pred)\n        f1 = f1_score(ground_truth, pred)\n\n        print(f\"Consumer: {consumer_id}\")\n        print(f\"Accuracy: {accuracy:.2f}\")\n        print(f\"Precision: {precision:.2f}\")\n        print(f\"Recall: {recall:.2f}\")\n        print(f\"F1 Score: {f1:.2f}\")\n        print(\"--------------\")\n\n        # Assuming total_accuracy, total_precision, total_recall, and total_f1 are lists that have been defined prior to the loop\n        total_accuracy.append(accuracy)\n        total_precision.append(precision)\n        total_recall.append(recall)\n        total_f1.append(f1)\n\n    print(f'Total accuracy: {sum(total_accuracy)/len(total_accuracy):.4f}')\n    print(f'Total precision: {sum(total_precision)/len(total_precision):.4f}')\n    print(f'Total recall: {sum(total_recall)/len(total_recall):.4f}')\n    print(f'Total f1: {sum(total_f1)/len(total_f1):.4f}')","block_group":"7bc17c7f04f94d4b93a8bb0b892d7946","execution_count":null,"outputs":[{"name":"stdout","text":"KMeans\n\n\nDataset: 1\nConsumer: 27396\nAccuracy: 0.63\nPrecision: 0.51\nRecall: 0.14\nF1 Score: 0.22\n--------------\nConsumer: 31709\nAccuracy: 0.57\nPrecision: 0.24\nRecall: 0.06\nF1 Score: 0.10\n--------------\nConsumer: 19373\nAccuracy: 0.57\nPrecision: 0.10\nRecall: 0.03\nF1 Score: 0.04\n--------------\nConsumer: 4691\nAccuracy: 0.55\nPrecision: 0.65\nRecall: 0.14\nF1 Score: 0.23\n--------------\nConsumer: 11612\nAccuracy: 0.50\nPrecision: 0.21\nRecall: 0.05\nF1 Score: 0.08\n--------------\nConsumer: 27966\nAccuracy: 0.70\nPrecision: 0.74\nRecall: 0.21\nF1 Score: 0.33\n--------------\nConsumer: 28310\nAccuracy: 0.57\nPrecision: 0.31\nRecall: 0.08\nF1 Score: 0.13\n--------------\nConsumer: 27391\nAccuracy: 0.47\nPrecision: 0.18\nRecall: 0.04\nF1 Score: 0.06\n--------------\nConsumer: 30199\nAccuracy: 0.58\nPrecision: 0.24\nRecall: 0.07\nF1 Score: 0.10\n--------------\nConsumer: 26952\nAccuracy: 0.65\nPrecision: 0.18\nRecall: 0.06\nF1 Score: 0.09\n--------------\nConsumer: 30894\nAccuracy: 0.55\nPrecision: 0.56\nRecall: 0.12\nF1 Score: 0.20\n--------------\nConsumer: 46170\nAccuracy: 0.58\nPrecision: 0.61\nRecall: 0.14\nF1 Score: 0.23\n--------------\nConsumer: 42635\nAccuracy: 0.52\nPrecision: 0.28\nRecall: 0.06\nF1 Score: 0.10\n--------------\nConsumer: 7020\nAccuracy: 0.77\nPrecision: 0.79\nRecall: 0.28\nF1 Score: 0.41\n--------------\nConsumer: 12791\nAccuracy: 0.59\nPrecision: 0.39\nRecall: 0.10\nF1 Score: 0.16\n--------------\nConsumer: 28961\nAccuracy: 0.86\nPrecision: 0.24\nRecall: 0.27\nF1 Score: 0.25\n--------------\nConsumer: 41436\nAccuracy: 0.69\nPrecision: 0.76\nRecall: 0.21\nF1 Score: 0.33\n--------------\nConsumer: 25414\nAccuracy: 0.51\nPrecision: 0.44\nRecall: 0.09\nF1 Score: 0.15\n--------------\nConsumer: 21496\nAccuracy: 0.90\nPrecision: 0.43\nRecall: 0.49\nF1 Score: 0.46\n--------------\nConsumer: 45806\nAccuracy: 0.58\nPrecision: 0.61\nRecall: 0.14\nF1 Score: 0.23\n--------------\nTotal accuracy: 0.6173\nTotal precision: 0.4229\nTotal recall: 0.1390\nTotal f1: 0.1952\nDataset: 2\nConsumer: 27396\nAccuracy: 0.79\nPrecision: 0.28\nRecall: 0.17\nF1 Score: 0.21\n--------------\nConsumer: 19373\nAccuracy: 0.89\nPrecision: 0.43\nRecall: 0.44\nF1 Score: 0.44\n--------------\nConsumer: 10042\nAccuracy: 0.87\nPrecision: 0.78\nRecall: 0.42\nF1 Score: 0.54\n--------------\nConsumer: 21532\nAccuracy: 0.91\nPrecision: 0.18\nRecall: 0.72\nF1 Score: 0.29\n--------------\nConsumer: 18786\nAccuracy: 0.78\nPrecision: 0.72\nRecall: 0.27\nF1 Score: 0.40\n--------------\nConsumer: 46170\nAccuracy: 0.66\nPrecision: 0.40\nRecall: 0.13\nF1 Score: 0.19\n--------------\nConsumer: 28639\nAccuracy: 0.82\nPrecision: 0.00\nRecall: 0.00\nF1 Score: 0.00\n--------------\nConsumer: 41383\nAccuracy: 0.59\nPrecision: 0.57\nRecall: 0.13\nF1 Score: 0.22\n--------------\nConsumer: 48747\nAccuracy: 0.81\nPrecision: 0.28\nRecall: 0.19\nF1 Score: 0.23\n--------------\nConsumer: 36438\nAccuracy: 0.67\nPrecision: 0.19\nRecall: 0.07\nF1 Score: 0.11\n--------------\nConsumer: 34715\nAccuracy: 0.74\nPrecision: 0.06\nRecall: 0.03\nF1 Score: 0.04\n--------------\nConsumer: 24746\nAccuracy: 0.76\nPrecision: 0.25\nRecall: 0.13\nF1 Score: 0.17\n--------------\nConsumer: 46526\nAccuracy: 0.50\nPrecision: 0.34\nRecall: 0.07\nF1 Score: 0.12\n--------------\nConsumer: 18245\nAccuracy: 0.86\nPrecision: 0.51\nRecall: 0.36\nF1 Score: 0.42\n--------------\nConsumer: 8355\nAccuracy: 0.46\nPrecision: 0.21\nRecall: 0.04\nF1 Score: 0.07\n--------------\nConsumer: 21496\nAccuracy: 0.56\nPrecision: 0.46\nRecall: 0.11\nF1 Score: 0.17\n--------------\nConsumer: 14105\nAccuracy: 0.85\nPrecision: 0.25\nRecall: 0.26\nF1 Score: 0.25\n--------------\nConsumer: 6465\nAccuracy: 0.85\nPrecision: 0.14\nRecall: 0.19\nF1 Score: 0.16\n--------------\nConsumer: 27095\nAccuracy: 0.71\nPrecision: 0.62\nRecall: 0.20\nF1 Score: 0.30\n--------------\nConsumer: 6770\nAccuracy: 0.88\nPrecision: 0.04\nRecall: 0.17\nF1 Score: 0.07\n--------------\nTotal accuracy: 0.6831\nTotal precision: 0.3793\nTotal recall: 0.1723\nTotal f1: 0.2078\nDataset: 3\nConsumer: 27396\nAccuracy: 0.58\nPrecision: 0.72\nRecall: 0.16\nF1 Score: 0.26\n--------------\nConsumer: 23361\nAccuracy: 0.87\nPrecision: 0.19\nRecall: 0.27\nF1 Score: 0.23\n--------------\nConsumer: 3832\nAccuracy: 0.87\nPrecision: 0.22\nRecall: 0.29\nF1 Score: 0.25\n--------------\nConsumer: 20985\nAccuracy: 0.63\nPrecision: 0.38\nRecall: 0.11\nF1 Score: 0.17\n--------------\nConsumer: 18786\nAccuracy: 0.94\nPrecision: 0.56\nRecall: 0.74\nF1 Score: 0.63\n--------------\nConsumer: 12636\nAccuracy: 0.48\nPrecision: 0.22\nRecall: 0.05\nF1 Score: 0.08\n--------------\nConsumer: 30894\nAccuracy: 0.97\nPrecision: 0.68\nRecall: 0.98\nF1 Score: 0.80\n--------------\nConsumer: 1318\nAccuracy: 0.84\nPrecision: 0.10\nRecall: 0.13\nF1 Score: 0.11\n--------------\nConsumer: 543\nAccuracy: 0.64\nPrecision: 0.51\nRecall: 0.14\nF1 Score: 0.22\n--------------\nConsumer: 4011\nAccuracy: 0.58\nPrecision: 0.25\nRecall: 0.07\nF1 Score: 0.11\n--------------\nConsumer: 38899\nAccuracy: 0.66\nPrecision: 0.50\nRecall: 0.15\nF1 Score: 0.23\n--------------\nConsumer: 7020\nAccuracy: 0.89\nPrecision: 0.61\nRecall: 0.46\nF1 Score: 0.52\n--------------\nConsumer: 8189\nAccuracy: 0.94\nPrecision: 0.56\nRecall: 0.74\nF1 Score: 0.63\n--------------\nConsumer: 20043\nAccuracy: 0.72\nPrecision: 0.43\nRecall: 0.16\nF1 Score: 0.23\n--------------\nConsumer: 27171\nAccuracy: 0.60\nPrecision: 0.18\nRecall: 0.05\nF1 Score: 0.08\n--------------\nConsumer: 27439\nAccuracy: 0.85\nPrecision: 0.10\nRecall: 0.14\nF1 Score: 0.11\n--------------\nConsumer: 21496\nAccuracy: 0.86\nPrecision: 0.18\nRecall: 0.25\nF1 Score: 0.21\n--------------\nConsumer: 28544\nAccuracy: 0.68\nPrecision: 0.57\nRecall: 0.17\nF1 Score: 0.26\n--------------\nConsumer: 7212\nAccuracy: 0.71\nPrecision: 0.39\nRecall: 0.14\nF1 Score: 0.21\n--------------\nConsumer: 10656\nAccuracy: 0.77\nPrecision: 0.01\nRecall: 0.01\nF1 Score: 0.01\n--------------\nTotal accuracy: 0.7063\nTotal precision: 0.3755\nTotal recall: 0.2017\nTotal f1: 0.2281\nDataset: 4\nConsumer: 25096\nAccuracy: 0.54\nPrecision: 0.38\nRecall: 0.09\nF1 Score: 0.14\n--------------\nConsumer: 19373\nAccuracy: 0.91\nPrecision: 0.08\nRecall: 1.00\nF1 Score: 0.15\n--------------\nConsumer: 23361\nAccuracy: 0.57\nPrecision: 0.58\nRecall: 0.13\nF1 Score: 0.21\n--------------\nConsumer: 29752\nAccuracy: 0.91\nPrecision: 0.21\nRecall: 0.68\nF1 Score: 0.32\n--------------\nConsumer: 27966\nAccuracy: 0.89\nPrecision: 0.78\nRecall: 0.47\nF1 Score: 0.59\n--------------\nConsumer: 1091\nAccuracy: 0.83\nPrecision: 0.11\nRecall: 0.12\nF1 Score: 0.12\n--------------\nConsumer: 18786\nAccuracy: 0.62\nPrecision: 0.75\nRecall: 0.18\nF1 Score: 0.28\n--------------\nConsumer: 42635\nAccuracy: 0.84\nPrecision: 0.14\nRecall: 0.16\nF1 Score: 0.15\n--------------\nConsumer: 543\nAccuracy: 0.66\nPrecision: 0.15\nRecall: 0.06\nF1 Score: 0.08\n--------------\nConsumer: 3799\nAccuracy: 0.65\nPrecision: 0.51\nRecall: 0.15\nF1 Score: 0.23\n--------------\nConsumer: 32895\nAccuracy: 0.53\nPrecision: 0.40\nRecall: 0.09\nF1 Score: 0.15\n--------------\nConsumer: 36438\nAccuracy: 0.93\nPrecision: 0.31\nRecall: 1.00\nF1 Score: 0.47\n--------------\nConsumer: 26637\nAccuracy: 0.91\nPrecision: 0.08\nRecall: 1.00\nF1 Score: 0.15\n--------------\nConsumer: 678\nAccuracy: 0.88\nPrecision: 0.67\nRecall: 0.45\nF1 Score: 0.54\n--------------\nConsumer: 31543\nAccuracy: 0.69\nPrecision: 0.75\nRecall: 0.21\nF1 Score: 0.33\n--------------\nConsumer: 1209\nAccuracy: 0.79\nPrecision: 0.76\nRecall: 0.29\nF1 Score: 0.42\n--------------\nConsumer: 18828\nAccuracy: 0.69\nPrecision: 0.76\nRecall: 0.21\nF1 Score: 0.33\n--------------\nConsumer: 45806\nAccuracy: 0.77\nPrecision: 0.10\nRecall: 0.07\nF1 Score: 0.08\n--------------\nConsumer: 10656\nAccuracy: 0.56\nPrecision: 0.15\nRecall: 0.04\nF1 Score: 0.07\n--------------\nConsumer: 2440\nAccuracy: 0.74\nPrecision: 0.01\nRecall: 0.01\nF1 Score: 0.01\n--------------\nTotal accuracy: 0.7162\nTotal precision: 0.3778\nTotal recall: 0.2313\nTotal f1: 0.2313\nDataset: 5\nConsumer: 19373\nAccuracy: 0.77\nPrecision: 0.28\nRecall: 0.15\nF1 Score: 0.20\n--------------\nConsumer: 31319\nAccuracy: 0.79\nPrecision: 0.39\nRecall: 0.21\nF1 Score: 0.27\n--------------\nConsumer: 10042\nAccuracy: 0.78\nPrecision: 0.00\nRecall: 0.00\nF1 Score: 0.00\n--------------\nConsumer: 38452\nAccuracy: 0.66\nPrecision: 0.65\nRecall: 0.18\nF1 Score: 0.28\n--------------\nConsumer: 26952\nAccuracy: 0.61\nPrecision: 0.11\nRecall: 0.04\nF1 Score: 0.05\n--------------\nConsumer: 1318\nAccuracy: 0.57\nPrecision: 0.65\nRecall: 0.14\nF1 Score: 0.23\n--------------\nConsumer: 32895\nAccuracy: 0.87\nPrecision: 0.08\nRecall: 0.18\nF1 Score: 0.11\n--------------\nConsumer: 7020\nAccuracy: 0.95\nPrecision: 0.47\nRecall: 1.00\nF1 Score: 0.64\n--------------\nConsumer: 49438\nAccuracy: 0.56\nPrecision: 0.15\nRecall: 0.04\nF1 Score: 0.07\n--------------\nConsumer: 678\nAccuracy: 0.94\nPrecision: 0.75\nRecall: 0.66\nF1 Score: 0.70\n--------------\nConsumer: 78\nAccuracy: 0.57\nPrecision: 0.25\nRecall: 0.07\nF1 Score: 0.10\n--------------\nConsumer: 5932\nAccuracy: 0.62\nPrecision: 0.19\nRecall: 0.06\nF1 Score: 0.09\n--------------\nConsumer: 48264\nAccuracy: 0.76\nPrecision: 0.26\nRecall: 0.14\nF1 Score: 0.18\n--------------\nConsumer: 27171\nAccuracy: 0.43\nPrecision: 0.00\nRecall: 0.00\nF1 Score: 0.00\n--------------\nConsumer: 2220\nAccuracy: 0.73\nPrecision: 0.74\nRecall: 0.23\nF1 Score: 0.36\n--------------\nConsumer: 21496\nAccuracy: 0.76\nPrecision: 0.86\nRecall: 0.28\nF1 Score: 0.42\n--------------\nConsumer: 28544\nAccuracy: 0.62\nPrecision: 0.49\nRecall: 0.13\nF1 Score: 0.20\n--------------\nConsumer: 14105\nAccuracy: 0.50\nPrecision: 0.21\nRecall: 0.05\nF1 Score: 0.08\n--------------\nConsumer: 43355\nAccuracy: 0.73\nPrecision: 0.12\nRecall: 0.06\nF1 Score: 0.08\n--------------\nConsumer: 45806\nAccuracy: 0.55\nPrecision: 0.49\nRecall: 0.11\nF1 Score: 0.18\n--------------\nTotal accuracy: 0.7106\nTotal precision: 0.3738\nTotal recall: 0.2222\nTotal f1: 0.2276\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":0},"cell_id":"2a99f8f9aedf4ea998dd9ea3ec7a0af1","deepnote_cell_type":"text-cell-h2"},"source":"## Elliptic Envelope","block_group":"01f1ae951f034cd583934d9a2e56dc55"},{"cell_type":"code","metadata":{"source_hash":"a5d370bd","execution_start":1699249926817,"execution_millis":776894,"deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":0},"deepnote_to_be_reexecuted":false,"cell_id":"0a2b0a8ef5ac4af1b6c6db6662747caf","deepnote_cell_type":"code"},"source":"count = 0\ntotal_accuracy = []\ntotal_precision = []\ntotal_recall = []\ntotal_f1 = []\nelliptic_pred = []\n\nprint('Elliptic Envelope\\n\\n')\n\nfor i in all_data:\n    count += 1\n    print(f'Dataset: {count}')\n    for j in range(len(i.index)):\n        consumer_data, consumer_id, fraud_point = consumerData(i, j)\n        fraud_point_week = (fraud_point/96)\n        weekly_consumption_year1, weekly_consumption_year2 = setupWeeklyData(consumer_data)\n\n        features = []\n        for week_data in weekly_consumption_year1 + weekly_consumption_year2:\n            result = catch22_all(week_data)\n            features.append(result['values'])\n\n        features = np.array(features)\n\n        # Check and handle NaN values \n        features = features[~np.isnan(features).any(axis=1)]\n\n        # Normalize the features\n        features = scaler.fit_transform(features)\n\n        # Applying PCA transformation\n        features = pca.fit_transform(features)\n        \n         # Using EllipticEnvelope (Minimum Covariance Determinant) for anomaly detection\n        envelope = EllipticEnvelope(contamination=0.1, support_fraction=0.8)\n        pred = envelope.fit_predict(features)\n        \n        # Convert predictions to match ground truth format: 1 for anomalies and 0 for normal\n        pred[pred == 1] = 0  # inliers (normal points)\n        pred[pred == -1] = 1  # outliers (anomalies)\n        elliptic_pred = pred\n        \n        # Get the ground truth\n        ground_truth = get_ground_truth(len(features), fraud_point_week)\n\n        accuracy = accuracy_score(ground_truth, pred)\n        precision = precision_score(ground_truth, pred)\n        recall = recall_score(ground_truth, pred)\n        f1 = f1_score(ground_truth, pred)\n\n        print(f\"Consumer: {consumer_id}\")\n        print(f\"Accuracy: {accuracy:.2f}\")\n        print(f\"Precision: {precision:.2f}\")\n        print(f\"Recall: {recall:.2f}\")\n        print(f\"F1 Score: {f1:.2f}\")\n        print(\"--------------\")\n\n        total_accuracy.append(accuracy)\n        total_precision.append(precision)\n        total_recall.append(recall)\n        total_f1.append(f1)\n\n        num_time_points = len(features)\n        num_features = features.shape[1]\n\n    print(f'Total accuracy: {sum(total_accuracy)/len(total_accuracy):.4f}', )\n    print(f'Total precision: {sum(total_precision)/len(total_precision):.4f}', )\n    print(f'Total recall: {sum(total_recall)/len(total_recall):.4f}', )\n    print(f'Total f1: {sum(total_f1)/len(total_f1):.4f}', )","block_group":"9e3afc3048cf4649bf6a4e4af415a0f2","execution_count":null,"outputs":[{"name":"stdout","text":"Elliptic Envelope\n\n\nDataset: 1\nConsumer: 27396\nAccuracy: 0.66\nPrecision: 0.68\nRecall: 0.18\nF1 Score: 0.29\n--------------\nConsumer: 31709\nAccuracy: 0.54\nPrecision: 0.06\nRecall: 0.01\nF1 Score: 0.02\n--------------\nConsumer: 19373\nAccuracy: 0.68\nPrecision: 0.62\nRecall: 0.18\nF1 Score: 0.28\n--------------\nConsumer: 4691\nAccuracy: 0.61\nPrecision: 0.94\nRecall: 0.20\nF1 Score: 0.33\n--------------\nConsumer: 11612\nAccuracy: 0.48\nPrecision: 0.12\nRecall: 0.03\nF1 Score: 0.05\n--------------\nConsumer: 27966\nAccuracy: 0.61\nPrecision: 0.31\nRecall: 0.09\nF1 Score: 0.14\n--------------\nConsumer: 28310\nAccuracy: 0.57\nPrecision: 0.28\nRecall: 0.07\nF1 Score: 0.11\n--------------\nConsumer: 27391\nAccuracy: 0.46\nPrecision: 0.11\nRecall: 0.02\nF1 Score: 0.04\n--------------\nConsumer: 30199\nAccuracy: 0.59\nPrecision: 0.26\nRecall: 0.07\nF1 Score: 0.11\n--------------\nConsumer: 26952\nAccuracy: 0.64\nPrecision: 0.12\nRecall: 0.04\nF1 Score: 0.06\n--------------\nConsumer: 30894\nAccuracy: 0.53\nPrecision: 0.46\nRecall: 0.10\nF1 Score: 0.16\n--------------\nConsumer: 46170\nAccuracy: 0.61\nPrecision: 0.75\nRecall: 0.17\nF1 Score: 0.28\n--------------\nConsumer: 42635\nAccuracy: 0.51\nPrecision: 0.22\nRecall: 0.05\nF1 Score: 0.08\n--------------\nConsumer: 7020\nAccuracy: 0.81\nPrecision: 0.99\nRecall: 0.34\nF1 Score: 0.51\n--------------\nConsumer: 12791\nAccuracy: 0.58\nPrecision: 0.36\nRecall: 0.09\nF1 Score: 0.15\n--------------\nConsumer: 28961\nAccuracy: 0.84\nPrecision: 0.17\nRecall: 0.19\nF1 Score: 0.18\n--------------\nConsumer: 41436\nAccuracy: 0.71\nPrecision: 0.86\nRecall: 0.24\nF1 Score: 0.37\n--------------\nConsumer: 25414\nAccuracy: 0.50\nPrecision: 0.39\nRecall: 0.08\nF1 Score: 0.14\n--------------\nConsumer: 21496\nAccuracy: 0.94\nPrecision: 0.62\nRecall: 0.71\nF1 Score: 0.67\n--------------\nConsumer: 45806\nAccuracy: 0.62\nPrecision: 0.82\nRecall: 0.19\nF1 Score: 0.30\n--------------\nTotal accuracy: 0.6243\nTotal precision: 0.4576\nTotal recall: 0.1539\nTotal f1: 0.2139\nDataset: 2\n/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:183: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-68.452453641767391 > -97.705376914528728). You may want to try with a higher value of support_fraction (current value: 0.799).\n  warnings.warn(\nConsumer: 27396\nAccuracy: 0.91\nPrecision: 0.89\nRecall: 0.53\nF1 Score: 0.67\n--------------\nConsumer: 19373\nAccuracy: 0.98\nPrecision: 0.88\nRecall: 0.90\nF1 Score: 0.89\n--------------\nConsumer: 10042\nAccuracy: 0.90\nPrecision: 0.92\nRecall: 0.49\nF1 Score: 0.64\n--------------\nConsumer: 21532\nAccuracy: 0.92\nPrecision: 0.22\nRecall: 0.89\nF1 Score: 0.36\n--------------\nConsumer: 18786\nAccuracy: 0.84\nPrecision: 1.00\nRecall: 0.38\nF1 Score: 0.55\n--------------\nConsumer: 46170\nAccuracy: 0.76\nPrecision: 0.88\nRecall: 0.28\nF1 Score: 0.42\n--------------\nConsumer: 28639\nAccuracy: 0.82\nPrecision: 0.00\nRecall: 0.00\nF1 Score: 0.00\n--------------\nConsumer: 41383\nAccuracy: 0.59\nPrecision: 0.61\nRecall: 0.14\nF1 Score: 0.23\n--------------\nConsumer: 48747\nAccuracy: 0.83\nPrecision: 0.35\nRecall: 0.24\nF1 Score: 0.29\n--------------\nConsumer: 36438\nAccuracy: 0.67\nPrecision: 0.19\nRecall: 0.07\nF1 Score: 0.11\n--------------\nConsumer: 34715\nAccuracy: 0.73\nPrecision: 0.01\nRecall: 0.01\nF1 Score: 0.01\n--------------\nConsumer: 24746\nAccuracy: 0.71\nPrecision: 0.00\nRecall: 0.00\nF1 Score: 0.00\n--------------\nConsumer: 46526\nAccuracy: 0.57\nPrecision: 0.68\nRecall: 0.15\nF1 Score: 0.24\n--------------\nConsumer: 18245\nAccuracy: 0.86\nPrecision: 0.54\nRecall: 0.38\nF1 Score: 0.45\n--------------\nConsumer: 8355\nAccuracy: 0.45\nPrecision: 0.17\nRecall: 0.03\nF1 Score: 0.06\n--------------\nConsumer: 21496\nAccuracy: 0.56\nPrecision: 0.43\nRecall: 0.10\nF1 Score: 0.16\n--------------\nConsumer: 14105\nAccuracy: 0.98\nPrecision: 0.90\nRecall: 0.93\nF1 Score: 0.92\n--------------\nConsumer: 6465\nAccuracy: 0.83\nPrecision: 0.03\nRecall: 0.04\nF1 Score: 0.03\n--------------\nConsumer: 27095\nAccuracy: 0.78\nPrecision: 1.00\nRecall: 0.32\nF1 Score: 0.48\n--------------\nConsumer: 6770\nAccuracy: 0.92\nPrecision: 0.25\nRecall: 1.00\nF1 Score: 0.40\n--------------\nTotal accuracy: 0.7028\nTotal precision: 0.4773\nTotal recall: 0.2491\nTotal f1: 0.2793\nDataset: 3\nConsumer: 27396\nAccuracy: 0.63\nPrecision: 1.00\nRecall: 0.21\nF1 Score: 0.35\n--------------\nConsumer: 23361\nAccuracy: 0.85\nPrecision: 0.11\nRecall: 0.15\nF1 Score: 0.13\n--------------\nConsumer: 3832\nAccuracy: 0.82\nPrecision: 0.00\nRecall: 0.00\nF1 Score: 0.00\n--------------\nConsumer: 20985\nAccuracy: 0.68\nPrecision: 0.60\nRecall: 0.18\nF1 Score: 0.27\n--------------\n/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:183: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-38.637349949331927 > -73.010929596886257). You may want to try with a higher value of support_fraction (current value: 0.799).\n  warnings.warn(\n/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:183: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-38.636033549346351 > -71.557016362718656). You may want to try with a higher value of support_fraction (current value: 0.799).\n  warnings.warn(\n/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:183: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-38.632233677520517 > -70.940171254543458). You may want to try with a higher value of support_fraction (current value: 0.799).\n  warnings.warn(\n/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:183: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-38.630197929485483 > -70.100655442265989). You may want to try with a higher value of support_fraction (current value: 0.799).\n  warnings.warn(\n/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:183: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-38.568013250789072 > -71.668859871765591). You may want to try with a higher value of support_fraction (current value: 0.799).\n  warnings.warn(\nConsumer: 18786\nAccuracy: 0.97\nPrecision: 0.72\nRecall: 0.96\nF1 Score: 0.83\n--------------\nConsumer: 12636\nAccuracy: 0.43\nPrecision: 0.00\nRecall: 0.00\nF1 Score: 0.00\n--------------\nConsumer: 30894\nAccuracy: 0.97\nPrecision: 0.69\nRecall: 1.00\nF1 Score: 0.82\n--------------\nConsumer: 1318\nAccuracy: 0.84\nPrecision: 0.10\nRecall: 0.13\nF1 Score: 0.11\n--------------\nConsumer: 543\nAccuracy: 0.63\nPrecision: 0.47\nRecall: 0.13\nF1 Score: 0.20\n--------------\nConsumer: 4011\nAccuracy: 0.54\nPrecision: 0.01\nRecall: 0.00\nF1 Score: 0.01\n--------------\nConsumer: 38899\nAccuracy: 0.59\nPrecision: 0.17\nRecall: 0.05\nF1 Score: 0.08\n--------------\nConsumer: 7020\nAccuracy: 0.87\nPrecision: 0.50\nRecall: 0.38\nF1 Score: 0.43\n--------------\nConsumer: 8189\nAccuracy: 0.96\nPrecision: 0.67\nRecall: 0.89\nF1 Score: 0.76\n--------------\nConsumer: 20043\nAccuracy: 0.80\nPrecision: 0.86\nRecall: 0.32\nF1 Score: 0.47\n--------------\nConsumer: 27171\nAccuracy: 0.60\nPrecision: 0.18\nRecall: 0.05\nF1 Score: 0.08\n--------------\nConsumer: 27439\nAccuracy: 0.97\nPrecision: 0.69\nRecall: 1.00\nF1 Score: 0.82\n--------------\nConsumer: 21496\nAccuracy: 0.94\nPrecision: 0.57\nRecall: 0.79\nF1 Score: 0.66\n--------------\nConsumer: 28544\nAccuracy: 0.65\nPrecision: 0.40\nRecall: 0.12\nF1 Score: 0.19\n--------------\nConsumer: 7212\nAccuracy: 0.82\nPrecision: 0.97\nRecall: 0.36\nF1 Score: 0.53\n--------------\nConsumer: 10656\nAccuracy: 0.77\nPrecision: 0.00\nRecall: 0.00\nF1 Score: 0.00\n--------------\nTotal accuracy: 0.7240\nTotal precision: 0.4636\nTotal recall: 0.2782\nTotal f1: 0.2984\nDataset: 4\nConsumer: 25096\nAccuracy: 0.54\nPrecision: 0.35\nRecall: 0.08\nF1 Score: 0.13\n--------------\nConsumer: 19373\nAccuracy: 0.89\nPrecision: 0.00\nRecall: 0.00\nF1 Score: 0.00\n--------------\nConsumer: 23361\nAccuracy: 0.51\nPrecision: 0.32\nRecall: 0.07\nF1 Score: 0.12\n--------------\nConsumer: 29752\nAccuracy: 0.91\nPrecision: 0.18\nRecall: 0.59\nF1 Score: 0.28\n--------------\n/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:183: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-87.479010487589932 > -88.433286033747038). You may want to try with a higher value of support_fraction (current value: 0.799).\n  warnings.warn(\n/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:183: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-87.416681691222578 > -87.900924763321427). You may want to try with a higher value of support_fraction (current value: 0.799).\n  warnings.warn(\n/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:183: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-86.498685394597459 > -86.941393472985425). You may want to try with a higher value of support_fraction (current value: 0.799).\n  warnings.warn(\n/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:183: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-87.829523016622829 > -88.508581916964019). You may want to try with a higher value of support_fraction (current value: 0.799).\n  warnings.warn(\n/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:183: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-87.479010487589932 > -89.160149003678910). You may want to try with a higher value of support_fraction (current value: 0.799).\n  warnings.warn(\n/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:183: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-87.829523016622829 > -89.027974236946235). You may want to try with a higher value of support_fraction (current value: 0.799).\n  warnings.warn(\n/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:183: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-87.479010487589932 > -87.518459372446983). You may want to try with a higher value of support_fraction (current value: 0.799).\n  warnings.warn(\n/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:183: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-86.498685394597459 > -86.735038476534129). You may want to try with a higher value of support_fraction (current value: 0.799).\n  warnings.warn(\n/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:183: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-87.829523016622829 > -89.196895662013802). You may want to try with a higher value of support_fraction (current value: 0.799).\n  warnings.warn(\n/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:183: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-87.829523016622829 > -88.523759550615324). You may want to try with a higher value of support_fraction (current value: 0.799).\n  warnings.warn(\n/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:183: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-86.316908961469196 > -87.451837806445610). You may want to try with a higher value of support_fraction (current value: 0.799).\n  warnings.warn(\n/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:183: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-53.683907196531166 > -84.511750224015969). You may want to try with a higher value of support_fraction (current value: 0.799).\n  warnings.warn(\nConsumer: 27966\nAccuracy: 0.94\nPrecision: 1.00\nRecall: 0.61\nF1 Score: 0.76\n--------------\nConsumer: 1091\nAccuracy: 0.85\nPrecision: 0.19\nRecall: 0.22\nF1 Score: 0.21\n--------------\nConsumer: 18786\nAccuracy: 0.66\nPrecision: 0.94\nRecall: 0.22\nF1 Score: 0.36\n--------------\nConsumer: 42635\nAccuracy: 0.82\nPrecision: 0.07\nRecall: 0.08\nF1 Score: 0.07\n--------------\nConsumer: 543\nAccuracy: 0.73\nPrecision: 0.50\nRecall: 0.19\nF1 Score: 0.27\n--------------\nConsumer: 3799\nAccuracy: 0.72\nPrecision: 0.89\nRecall: 0.25\nF1 Score: 0.39\n--------------\nConsumer: 32895\nAccuracy: 0.60\nPrecision: 0.74\nRecall: 0.16\nF1 Score: 0.27\n--------------\nConsumer: 36438\nAccuracy: 0.88\nPrecision: 0.07\nRecall: 0.23\nF1 Score: 0.11\n--------------\nConsumer: 26637\nAccuracy: 0.91\nPrecision: 0.08\nRecall: 1.00\nF1 Score: 0.15\n--------------\nConsumer: 678\nAccuracy: 0.90\nPrecision: 0.76\nRecall: 0.51\nF1 Score: 0.61\n--------------\nConsumer: 31543\nAccuracy: 0.72\nPrecision: 0.90\nRecall: 0.25\nF1 Score: 0.40\n--------------\nConsumer: 1209\nAccuracy: 0.80\nPrecision: 0.81\nRecall: 0.31\nF1 Score: 0.44\n--------------\nConsumer: 18828\nAccuracy: 0.69\nPrecision: 0.76\nRecall: 0.21\nF1 Score: 0.33\n--------------\nConsumer: 45806\nAccuracy: 0.77\nPrecision: 0.08\nRecall: 0.06\nF1 Score: 0.07\n--------------\nConsumer: 10656\nAccuracy: 0.63\nPrecision: 0.50\nRecall: 0.14\nF1 Score: 0.22\n--------------\nConsumer: 2440\nAccuracy: 0.74\nPrecision: 0.01\nRecall: 0.01\nF1 Score: 0.01\n--------------\nTotal accuracy: 0.7331\nTotal precision: 0.4623\nTotal recall: 0.2735\nTotal f1: 0.2887\nDataset: 5\nConsumer: 19373\nAccuracy: 0.90\nPrecision: 0.90\nRecall: 0.49\nF1 Score: 0.63\n--------------\nConsumer: 31319\nAccuracy: 0.78\nPrecision: 0.32\nRecall: 0.17\nF1 Score: 0.22\n--------------\nConsumer: 10042\nAccuracy: 0.79\nPrecision: 0.03\nRecall: 0.02\nF1 Score: 0.03\n--------------\nConsumer: 38452\nAccuracy: 0.70\nPrecision: 0.83\nRecall: 0.23\nF1 Score: 0.36\n--------------\nConsumer: 26952\nAccuracy: 0.62\nPrecision: 0.14\nRecall: 0.04\nF1 Score: 0.07\n--------------\nConsumer: 1318\nAccuracy: 0.57\nPrecision: 0.67\nRecall: 0.14\nF1 Score: 0.24\n--------------\nConsumer: 32895\nAccuracy: 0.95\nPrecision: 0.47\nRecall: 1.00\nF1 Score: 0.64\n--------------\nConsumer: 7020\nAccuracy: 0.95\nPrecision: 0.47\nRecall: 1.00\nF1 Score: 0.64\n--------------\nConsumer: 49438\nAccuracy: 0.57\nPrecision: 0.18\nRecall: 0.05\nF1 Score: 0.08\n--------------\n/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:183: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-126.041746033328593 > -156.442716785535424). You may want to try with a higher value of support_fraction (current value: 0.799).\n  warnings.warn(\n/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:183: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-126.316352460793183 > -155.972311970706272). You may want to try with a higher value of support_fraction (current value: 0.799).\n  warnings.warn(\n/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py:183: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-127.136555205906745 > -153.319250714429643). You may want to try with a higher value of support_fraction (current value: 0.799).\n  warnings.warn(\nConsumer: 678\nAccuracy: 0.95\nPrecision: 0.81\nRecall: 0.71\nF1 Score: 0.75\n--------------\nConsumer: 78\nAccuracy: 0.60\nPrecision: 0.40\nRecall: 0.11\nF1 Score: 0.17\n--------------\nConsumer: 5932\nAccuracy: 0.68\nPrecision: 0.50\nRecall: 0.16\nF1 Score: 0.24\n--------------\nConsumer: 48264\nAccuracy: 0.85\nPrecision: 0.75\nRecall: 0.39\nF1 Score: 0.51\n--------------\nConsumer: 27171\nAccuracy: 0.43\nPrecision: 0.00\nRecall: 0.00\nF1 Score: 0.00\n--------------\nConsumer: 2220\nAccuracy: 0.74\nPrecision: 0.76\nRecall: 0.24\nF1 Score: 0.37\n--------------\nConsumer: 21496\nAccuracy: 0.79\nPrecision: 1.00\nRecall: 0.32\nF1 Score: 0.49\n--------------\nConsumer: 28544\nAccuracy: 0.56\nPrecision: 0.18\nRecall: 0.05\nF1 Score: 0.08\n--------------\nConsumer: 14105\nAccuracy: 0.54\nPrecision: 0.42\nRecall: 0.09\nF1 Score: 0.15\n--------------\nConsumer: 43355\nAccuracy: 0.71\nPrecision: 0.04\nRecall: 0.02\nF1 Score: 0.03\n--------------\nConsumer: 45806\nAccuracy: 0.64\nPrecision: 0.92\nRecall: 0.21\nF1 Score: 0.34\n--------------\nTotal accuracy: 0.7295\nTotal precision: 0.4677\nTotal recall: 0.2733\nTotal f1: 0.2913\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":0},"cell_id":"3bed8b3d3e2c43fd8fe2cc73b6e4c1dc","deepnote_cell_type":"text-cell-h2"},"source":"## Autoencoders","block_group":"7d0db859a67145e09d7f82d78a7801ca"},{"cell_type":"code","metadata":{"source_hash":"91c5b53","execution_start":1699250703724,"execution_millis":758796,"deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":0},"deepnote_to_be_reexecuted":false,"cell_id":"56484967805e49e4ace38833ba0f76b7","deepnote_cell_type":"code"},"source":"count = 0\ntotal_accuracy = []\ntotal_precision = []\ntotal_recall = []\ntotal_f1 = []\nautoencoder_pred = []\n\nprint('Autoencoders\\n\\n')\n\nfor i in all_data:\n    count += 1\n    print(f'Dataset: {count}')\n    for j in range(len(i.index)):\n        consumer_data, consumer_id, fraud_point = consumerData(i, j)\n        fraud_point_week = (fraud_point/96)\n        weekly_consumption_year1, weekly_consumption_year2 = setupWeeklyData(consumer_data)\n\n        features = []\n        for week_data in weekly_consumption_year1 + weekly_consumption_year2:\n            result = catch22_all(week_data)\n            features.append(result['values'])\n\n        features = np.array(features)\n\n        # Check and handle NaN values \n        features = features[~np.isnan(features).any(axis=1)]\n\n        # Normalize the features\n        features = scaler.fit_transform(features)\n\n        # Applying PCA transformation\n        features = pca.fit_transform(features)\n        \n        input_dim = features.shape[1]\n        encoding_dim = int(input_dim / 2)\n\n\n        input_layer = Input(shape=(input_dim,))\n        encoder = Dense(encoding_dim, activation=\"tanh\")(input_layer)\n        decoder = Dense(input_dim, activation=\"sigmoid\")(encoder)\n\n        autoencoder = Model(inputs=input_layer, outputs=decoder)\n        autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss=\"mean_squared_error\")\n        \n        # Train the autoencoder\n        autoencoder.fit(features, features, epochs=50, batch_size=20, validation_split=0.1, verbose=0)\n\n        # Use the autoencoder to reconstruct the input\n        predictions = autoencoder.predict(features)\n\n        # Calculate reconstruction errors\n        mse = np.mean(np.power(features - predictions, 2), axis=1)\n        threshold = np.quantile(mse, 0.95)  # Here, we're setting a threshold at the 95th percentile of the error.\n        pred = [1 if error > threshold else 0 for error in mse]\n        autoencoder_pred = pred\n\n        # Get the ground truth\n        ground_truth = get_ground_truth(len(features), fraud_point_week)\n\n        accuracy = accuracy_score(ground_truth, pred)\n        precision = precision_score(ground_truth, pred)\n        recall = recall_score(ground_truth, pred)\n        f1 = f1_score(ground_truth, pred)\n\n        print(f\"Consumer: {consumer_id}\")\n        print(f\"Accuracy: {accuracy:.2f}\")\n        print(f\"Precision: {precision:.2f}\")\n        print(f\"Recall: {recall:.2f}\")\n        print(f\"F1 Score: {f1:.2f}\")\n        print(\"--------------\")\n\n        total_accuracy.append(accuracy)\n        total_precision.append(precision)\n        total_recall.append(recall)\n        total_f1.append(f1)\n\n        num_time_points = len(features)\n        num_features = features.shape[1]\n\n    print(f'Total accuracy: {sum(total_accuracy)/len(total_accuracy):.4f}', )\n    print(f'Total precision: {sum(total_precision)/len(total_precision):.4f}', )\n    print(f'Total recall: {sum(total_recall)/len(total_recall):.4f}', )\n    print(f'Total f1: {sum(total_f1)/len(total_f1):.4f}', )","block_group":"d7df184f61254172a23afcf2b4137437","execution_count":null,"outputs":[{"name":"stdout","text":"Autoencoders\n\n\nDataset: 1\n2023-11-06 06:05:06.714955: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n2023-11-06 06:05:06.714990: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n2023-11-06 06:05:06.715011: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (p-d7f56da1-e665-45c2-b1a9-a2c9d93d2a27): /proc/driver/nvidia/version does not exist\n2023-11-06 06:05:06.715323: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n23/23 [==============================] - 0s 837us/step\nConsumer: 27396\nAccuracy: 0.62\nPrecision: 0.47\nRecall: 0.06\nF1 Score: 0.11\n--------------\n23/23 [==============================] - 0s 983us/step\nConsumer: 31709\nAccuracy: 0.61\nPrecision: 0.39\nRecall: 0.05\nF1 Score: 0.09\n--------------\n23/23 [==============================] - 0s 850us/step\nConsumer: 19373\nAccuracy: 0.64\nPrecision: 0.39\nRecall: 0.06\nF1 Score: 0.10\n--------------\n23/23 [==============================] - 0s 901us/step\nConsumer: 4691\nAccuracy: 0.56\nPrecision: 0.86\nRecall: 0.09\nF1 Score: 0.16\n--------------\n23/23 [==============================] - 0s 1ms/step\nConsumer: 11612\nAccuracy: 0.51\nPrecision: 0.06\nRecall: 0.01\nF1 Score: 0.01\n--------------\n23/23 [==============================] - 0s 927us/step\nConsumer: 27966\nAccuracy: 0.68\nPrecision: 0.78\nRecall: 0.11\nF1 Score: 0.20\n--------------\n23/23 [==============================] - 0s 1ms/step\nConsumer: 28310\nAccuracy: 0.57\nPrecision: 0.08\nRecall: 0.01\nF1 Score: 0.02\n--------------\n23/23 [==============================] - 0s 2ms/step\nConsumer: 27391\nAccuracy: 0.49\nPrecision: 0.00\nRecall: 0.00\nF1 Score: 0.00\n--------------\n23/23 [==============================] - 0s 1ms/step\nConsumer: 30199\nAccuracy: 0.59\nPrecision: 0.03\nRecall: 0.00\nF1 Score: 0.01\n--------------\n23/23 [==============================] - 0s 1ms/step\nConsumer: 26952\nAccuracy: 0.67\nPrecision: 0.14\nRecall: 0.02\nF1 Score: 0.04\n--------------\n23/23 [==============================] - 0s 922us/step\nConsumer: 30894\nAccuracy: 0.56\nPrecision: 0.75\nRecall: 0.08\nF1 Score: 0.15\n--------------\n23/23 [==============================] - 0s 821us/step\nConsumer: 46170\nAccuracy: 0.58\nPrecision: 0.69\nRecall: 0.08\nF1 Score: 0.14\n--------------\n23/23 [==============================] - 0s 807us/step\nConsumer: 42635\nAccuracy: 0.54\nPrecision: 0.31\nRecall: 0.04\nF1 Score: 0.06\n--------------\n23/23 [==============================] - 0s 839us/step\nConsumer: 7020\nAccuracy: 0.75\nPrecision: 0.89\nRecall: 0.15\nF1 Score: 0.26\n--------------\n23/23 [==============================] - 0s 1ms/step\nConsumer: 12791\nAccuracy: 0.63\nPrecision: 0.64\nRecall: 0.08\nF1 Score: 0.15\n--------------\n23/23 [==============================] - 0s 783us/step\nConsumer: 28961\nAccuracy: 0.88\nPrecision: 0.17\nRecall: 0.10\nF1 Score: 0.12\n--------------\n23/23 [==============================] - 0s 907us/step\nConsumer: 41436\nAccuracy: 0.67\nPrecision: 0.83\nRecall: 0.12\nF1 Score: 0.20\n--------------\n23/23 [==============================] - 0s 769us/step\nConsumer: 25414\nAccuracy: 0.54\nPrecision: 0.72\nRecall: 0.08\nF1 Score: 0.14\n--------------\n23/23 [==============================] - 0s 799us/step\nConsumer: 21496\nAccuracy: 0.90\nPrecision: 0.36\nRecall: 0.21\nF1 Score: 0.26\n--------------\n23/23 [==============================] - 0s 785us/step\nConsumer: 45806\nAccuracy: 0.58\nPrecision: 0.69\nRecall: 0.08\nF1 Score: 0.14\n--------------\nTotal accuracy: 0.6291\nTotal precision: 0.4625\nTotal recall: 0.0713\nTotal f1: 0.1185\nDataset: 2\n23/23 [==============================] - 0s 1ms/step\nConsumer: 27396\nAccuracy: 0.85\nPrecision: 0.64\nRecall: 0.19\nF1 Score: 0.29\n--------------\n23/23 [==============================] - 0s 1ms/step\nConsumer: 19373\nAccuracy: 0.93\nPrecision: 0.78\nRecall: 0.40\nF1 Score: 0.53\n--------------\n23/23 [==============================] - 0s 1ms/step\nConsumer: 10042\nAccuracy: 0.85\nPrecision: 0.89\nRecall: 0.24\nF1 Score: 0.38\n--------------\n23/23 [==============================] - 0s 2ms/step\nConsumer: 21532\nAccuracy: 0.94\nPrecision: 0.19\nRecall: 0.39\nF1 Score: 0.26\n--------------\n23/23 [==============================] - 0s 756us/step\nConsumer: 18786\nAccuracy: 0.78\nPrecision: 1.00\nRecall: 0.19\nF1 Score: 0.32\n--------------\n23/23 [==============================] - 0s 1ms/step\nConsumer: 46170\nAccuracy: 0.70\nPrecision: 0.67\nRecall: 0.11\nF1 Score: 0.18\n--------------\n23/23 [==============================] - 0s 757us/step\nConsumer: 28639\nAccuracy: 0.87\nPrecision: 0.00\nRecall: 0.00\nF1 Score: 0.00\n--------------\n23/23 [==============================] - 0s 810us/step\nConsumer: 41383\nAccuracy: 0.59\nPrecision: 0.64\nRecall: 0.07\nF1 Score: 0.13\n--------------\n23/23 [==============================] - 0s 1ms/step\nConsumer: 48747\nAccuracy: 0.83\nPrecision: 0.28\nRecall: 0.10\nF1 Score: 0.14\n--------------\n23/23 [==============================] - 0s 884us/step\nConsumer: 36438\nAccuracy: 0.70\nPrecision: 0.19\nRecall: 0.04\nF1 Score: 0.06\n--------------\n23/23 [==============================] - 0s 1ms/step\nConsumer: 34715\nAccuracy: 0.82\nPrecision: 0.39\nRecall: 0.12\nF1 Score: 0.18\n--------------\n23/23 [==============================] - 0s 1ms/step\nConsumer: 24746\nAccuracy: 0.80\nPrecision: 0.33\nRecall: 0.09\nF1 Score: 0.14\n--------------\n22/22 [==============================] - 0s 942us/step\nConsumer: 46526\nAccuracy: 0.51\nPrecision: 0.31\nRecall: 0.03\nF1 Score: 0.06\n--------------\n23/23 [==============================] - 0s 739us/step\nConsumer: 18245\nAccuracy: 0.86\nPrecision: 0.56\nRecall: 0.19\nF1 Score: 0.29\n--------------\n23/23 [==============================] - 0s 1ms/step\nConsumer: 8355\nAccuracy: 0.48\nPrecision: 0.08\nRecall: 0.01\nF1 Score: 0.02\n--------------\n23/23 [==============================] - 0s 875us/step\nConsumer: 21496\nAccuracy: 0.57\nPrecision: 0.44\nRecall: 0.05\nF1 Score: 0.09\n--------------\n23/23 [==============================] - 0s 741us/step\nConsumer: 14105\nAccuracy: 0.94\nPrecision: 0.89\nRecall: 0.46\nF1 Score: 0.60\n--------------\n23/23 [==============================] - 0s 947us/step\nConsumer: 6465\nAccuracy: 0.88\nPrecision: 0.03\nRecall: 0.02\nF1 Score: 0.02\n--------------\n23/23 [==============================] - 0s 920us/step\nConsumer: 27095\nAccuracy: 0.73\nPrecision: 0.92\nRecall: 0.15\nF1 Score: 0.25\n--------------\n23/23 [==============================] - 0s 1ms/step\nConsumer: 6770\nAccuracy: 0.97\nPrecision: 0.50\nRecall: 1.00\nF1 Score: 0.67\n--------------\nTotal accuracy: 0.7048\nTotal precision: 0.4743\nTotal recall: 0.1316\nTotal f1: 0.1748\nDataset: 3\n23/23 [==============================] - 0s 1ms/step\nConsumer: 27396\nAccuracy: 0.55\nPrecision: 0.72\nRecall: 0.08\nF1 Score: 0.14\n--------------\n23/23 [==============================] - 0s 861us/step\nConsumer: 23361\nAccuracy: 0.90\nPrecision: 0.22\nRecall: 0.15\nF1 Score: 0.18\n--------------\n23/23 [==============================] - 0s 1ms/step\nConsumer: 3832\nAccuracy: 0.93\nPrecision: 0.53\nRecall: 0.35\nF1 Score: 0.42\n--------------\n23/23 [==============================] - 0s 913us/step\nConsumer: 20985\nAccuracy: 0.65\nPrecision: 0.36\nRecall: 0.05\nF1 Score: 0.09\n--------------\n23/23 [==============================] - 0s 806us/step\nConsumer: 18786\nAccuracy: 0.94\nPrecision: 0.69\nRecall: 0.46\nF1 Score: 0.56\n--------------\n23/23 [==============================] - 0s 1ms/step\nConsumer: 12636\nAccuracy: 0.50\nPrecision: 0.14\nRecall: 0.01\nF1 Score: 0.03\n--------------\n23/23 [==============================] - 0s 2ms/step\nConsumer: 30894\nAccuracy: 0.97\nPrecision: 0.86\nRecall: 0.62\nF1 Score: 0.72\n--------------\n23/23 [==============================] - 0s 1ms/step\nConsumer: 1318\nAccuracy: 0.88\nPrecision: 0.11\nRecall: 0.07\nF1 Score: 0.09\n--------------\n23/23 [==============================] - 0s 2ms/step\nConsumer: 543\nAccuracy: 0.64\nPrecision: 0.58\nRecall: 0.08\nF1 Score: 0.14\n--------------\n23/23 [==============================] - 0s 789us/step\nConsumer: 4011\nAccuracy: 0.63\nPrecision: 0.42\nRecall: 0.06\nF1 Score: 0.10\n--------------\n23/23 [==============================] - 0s 764us/step\nConsumer: 38899\nAccuracy: 0.66\nPrecision: 0.56\nRecall: 0.08\nF1 Score: 0.14\n--------------\n23/23 [==============================] - 0s 874us/step\nConsumer: 7020\nAccuracy: 0.88\nPrecision: 0.67\nRecall: 0.25\nF1 Score: 0.36\n--------------\n23/23 [==============================] - 0s 839us/step\nConsumer: 8189\nAccuracy: 0.92\nPrecision: 0.42\nRecall: 0.28\nF1 Score: 0.33\n--------------\n23/23 [==============================] - 0s 776us/step\nConsumer: 20043\nAccuracy: 0.76\nPrecision: 0.81\nRecall: 0.15\nF1 Score: 0.25\n--------------\n23/23 [==============================] - 0s 1ms/step\nConsumer: 27171\nAccuracy: 0.63\nPrecision: 0.11\nRecall: 0.02\nF1 Score: 0.03\n--------------\n23/23 [==============================] - 0s 782us/step\nConsumer: 27439\nAccuracy: 0.95\nPrecision: 0.69\nRecall: 0.50\nF1 Score: 0.58\n--------------\n23/23 [==============================] - 0s 2ms/step\nConsumer: 21496\nAccuracy: 0.94\nPrecision: 0.64\nRecall: 0.44\nF1 Score: 0.52\n--------------\n23/23 [==============================] - 0s 2ms/step\nConsumer: 28544\nAccuracy: 0.67\nPrecision: 0.53\nRecall: 0.08\nF1 Score: 0.14\n--------------\n23/23 [==============================] - 0s 1ms/step\nConsumer: 7212\nAccuracy: 0.75\nPrecision: 0.72\nRecall: 0.13\nF1 Score: 0.23\n--------------\n23/23 [==============================] - 0s 908us/step\nConsumer: 10656\nAccuracy: 0.82\nPrecision: 0.00\nRecall: 0.00\nF1 Score: 0.00\n--------------\nTotal accuracy: 0.7293\nTotal precision: 0.4792\nTotal recall: 0.1523\nTotal f1: 0.2008\nDataset: 4\n23/23 [==============================] - 0s 738us/step\nConsumer: 25096\nAccuracy: 0.59\nPrecision: 0.67\nRecall: 0.08\nF1 Score: 0.14\n--------------\n23/23 [==============================] - 0s 919us/step\nConsumer: 19373\nAccuracy: 0.95\nPrecision: 0.11\nRecall: 0.67\nF1 Score: 0.19\n--------------\n23/23 [==============================] - 0s 1ms/step\nConsumer: 23361\nAccuracy: 0.57\nPrecision: 0.69\nRecall: 0.08\nF1 Score: 0.14\n--------------\n23/23 [==============================] - 0s 1ms/step\nConsumer: 29752\nAccuracy: 0.93\nPrecision: 0.14\nRecall: 0.23\nF1 Score: 0.17\n--------------\n23/23 [==============================] - 0s 848us/step\nConsumer: 27966\nAccuracy: 0.89\nPrecision: 1.00\nRecall: 0.31\nF1 Score: 0.47\n--------------\n23/23 [==============================] - 0s 1ms/step\nConsumer: 1091\nAccuracy: 0.87\nPrecision: 0.11\nRecall: 0.06\nF1 Score: 0.08\n--------------\n23/23 [==============================] - 0s 925us/step\nConsumer: 18786\nAccuracy: 0.60\nPrecision: 0.81\nRecall: 0.09\nF1 Score: 0.17\n--------------\n23/23 [==============================] - 0s 1ms/step\nConsumer: 42635\nAccuracy: 0.88\nPrecision: 0.22\nRecall: 0.12\nF1 Score: 0.16\n--------------\n23/23 [==============================] - 0s 1ms/step\nConsumer: 543\nAccuracy: 0.76\nPrecision: 0.78\nRecall: 0.15\nF1 Score: 0.25\n--------------\n23/23 [==============================] - 0s 951us/step\nConsumer: 3799\nAccuracy: 0.69\nPrecision: 0.94\nRecall: 0.13\nF1 Score: 0.23\n--------------\n23/23 [==============================] - 0s 1ms/step\nConsumer: 32895\nAccuracy: 0.54\nPrecision: 0.42\nRecall: 0.05\nF1 Score: 0.08\n--------------\n23/23 [==============================] - 0s 2ms/step\nConsumer: 36438\nAccuracy: 0.96\nPrecision: 0.36\nRecall: 0.59\nF1 Score: 0.45\n--------------\n23/23 [==============================] - 0s 787us/step\nConsumer: 26637\nAccuracy: 0.96\nPrecision: 0.17\nRecall: 1.00\nF1 Score: 0.29\n--------------\n23/23 [==============================] - 0s 788us/step\nConsumer: 678\nAccuracy: 0.87\nPrecision: 0.72\nRecall: 0.24\nF1 Score: 0.36\n--------------\n23/23 [==============================] - 0s 1ms/step\nConsumer: 31543\nAccuracy: 0.65\nPrecision: 0.58\nRecall: 0.08\nF1 Score: 0.14\n--------------\n23/23 [==============================] - 0s 1ms/step\nConsumer: 1209\nAccuracy: 0.77\nPrecision: 0.81\nRecall: 0.15\nF1 Score: 0.26\n--------------\n23/23 [==============================] - 0s 1ms/step\nConsumer: 18828\nAccuracy: 0.68\nPrecision: 1.00\nRecall: 0.14\nF1 Score: 0.24\n--------------\n23/23 [==============================] - 0s 1ms/step\nConsumer: 45806\nAccuracy: 0.90\nPrecision: 0.97\nRecall: 0.33\nF1 Score: 0.49\n--------------\n23/23 [==============================] - 0s 805us/step\nConsumer: 10656\nAccuracy: 0.61\nPrecision: 0.22\nRecall: 0.03\nF1 Score: 0.05\n--------------\n23/23 [==============================] - 0s 1ms/step\nConsumer: 2440\nAccuracy: 0.87\nPrecision: 0.83\nRecall: 0.25\nF1 Score: 0.39\n--------------\nTotal accuracy: 0.7411\nTotal precision: 0.5038\nTotal recall: 0.1740\nTotal f1: 0.2100\nDataset: 5\n23/23 [==============================] - 0s 748us/step\nConsumer: 19373\nAccuracy: 0.84\nPrecision: 0.72\nRecall: 0.20\nF1 Score: 0.31\n--------------\n23/23 [==============================] - 0s 1ms/step\nConsumer: 31319\nAccuracy: 0.80\nPrecision: 0.39\nRecall: 0.11\nF1 Score: 0.17\n--------------\n23/23 [==============================] - 0s 1ms/step\nConsumer: 10042\nAccuracy: 0.90\nPrecision: 0.61\nRecall: 0.27\nF1 Score: 0.37\n--------------\n23/23 [==============================] - 0s 2ms/step\nConsumer: 38452\nAccuracy: 0.63\nPrecision: 0.53\nRecall: 0.07\nF1 Score: 0.13\n--------------\n23/23 [==============================] - 0s 838us/step\nConsumer: 26952\nAccuracy: 0.65\nPrecision: 0.11\nRecall: 0.02\nF1 Score: 0.03\n--------------\n23/23 [==============================] - 0s 854us/step\nConsumer: 1318\nAccuracy: 0.57\nPrecision: 0.81\nRecall: 0.09\nF1 Score: 0.16\n--------------\n23/23 [==============================] - 0s 807us/step\nConsumer: 32895\nAccuracy: 0.98\nPrecision: 0.81\nRecall: 0.85\nF1 Score: 0.83\n--------------\n23/23 [==============================] - 0s 801us/step\nConsumer: 7020\nAccuracy: 0.99\nPrecision: 0.83\nRecall: 0.88\nF1 Score: 0.86\n--------------\n23/23 [==============================] - 0s 784us/step\nConsumer: 49438\nAccuracy: 0.61\nPrecision: 0.33\nRecall: 0.05\nF1 Score: 0.08\n--------------\n23/23 [==============================] - 0s 3ms/step\nConsumer: 678\nAccuracy: 0.93\nPrecision: 0.94\nRecall: 0.41\nF1 Score: 0.58\n--------------\n23/23 [==============================] - 0s 891us/step\nConsumer: 78\nAccuracy: 0.58\nPrecision: 0.08\nRecall: 0.01\nF1 Score: 0.02\n--------------\n23/23 [==============================] - 0s 824us/step\nConsumer: 5932\nAccuracy: 0.65\nPrecision: 0.19\nRecall: 0.03\nF1 Score: 0.05\n--------------\n23/23 [==============================] - 0s 750us/step\nConsumer: 48264\nAccuracy: 0.80\nPrecision: 0.44\nRecall: 0.11\nF1 Score: 0.18\n--------------\n23/23 [==============================] - 0s 1ms/step\nConsumer: 27171\nAccuracy: 0.48\nPrecision: 0.00\nRecall: 0.00\nF1 Score: 0.00\n--------------\n23/23 [==============================] - 0s 816us/step\nConsumer: 2220\nAccuracy: 0.69\nPrecision: 0.58\nRecall: 0.09\nF1 Score: 0.16\n--------------\n23/23 [==============================] - 0s 828us/step\nConsumer: 21496\nAccuracy: 0.74\nPrecision: 1.00\nRecall: 0.16\nF1 Score: 0.28\n--------------\n23/23 [==============================] - 0s 865us/step\nConsumer: 28544\nAccuracy: 0.65\nPrecision: 0.78\nRecall: 0.10\nF1 Score: 0.18\n--------------\n23/23 [==============================] - 0s 745us/step\nConsumer: 14105\nAccuracy: 0.53\nPrecision: 0.22\nRecall: 0.03\nF1 Score: 0.05\n--------------\n23/23 [==============================] - 0s 1ms/step\nConsumer: 43355\nAccuracy: 0.80\nPrecision: 0.44\nRecall: 0.11\nF1 Score: 0.18\n--------------\n23/23 [==============================] - 0s 822us/step\nConsumer: 45806\nAccuracy: 0.57\nPrecision: 0.61\nRecall: 0.07\nF1 Score: 0.12\n--------------\nTotal accuracy: 0.7368\nTotal precision: 0.5075\nTotal recall: 0.1758\nTotal f1: 0.2153\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":0},"cell_id":"79b62286089949feb725a6ab4ffa6664","deepnote_cell_type":"text-cell-h2"},"source":"## Agglomerative Clustering","block_group":"7ddb5c0495f240b1800a1468bf86ce82"},{"cell_type":"code","metadata":{"source_hash":"47f55faa","execution_start":1699251462533,"execution_millis":287584,"deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":0},"deepnote_to_be_reexecuted":false,"cell_id":"44cedd6c7f7c4210a409ca7f3f1ac769","deepnote_cell_type":"code"},"source":"count = 0\ntotal_accuracy = []\ntotal_precision = []\ntotal_recall = []\ntotal_f1 = []\naggclust_pred = []\n\nprint('Agglomerative Clustering\\n\\n')\n\nfor i in all_data:\n    count += 1\n    print(f'Dataset: {count}')\n    for j in range(len(i.index)):\n        consumer_data, consumer_id, fraud_point = consumerData(i, j)\n        fraud_point_week = (fraud_point/96)\n        weekly_consumption_year1, weekly_consumption_year2 = setupWeeklyData(consumer_data)\n\n        features = []\n        for week_data in weekly_consumption_year1 + weekly_consumption_year2:\n            result = catch22_all(week_data)\n            features.append(result['values'])\n\n        features = np.array(features)\n\n        # Check and handle NaN values \n        features = features[~np.isnan(features).any(axis=1)]\n\n        # Normalize the features\n        features = scaler.fit_transform(features)\n\n        # Applying PCA transformation\n        features = pca.fit_transform(features)\n        \n        # Using Agglomerative Clustering\n        clustering = AgglomerativeClustering(n_clusters=2).fit(features)\n\n        # Assuming the last cluster is anomalous\n        pred = [1 if label == np.max(clustering.labels_) else 0 for label in clustering.labels_]\n        aggclust_pred = pred\n\n        # Get the ground truth\n        ground_truth = get_ground_truth(len(features), fraud_point_week)\n\n        accuracy = accuracy_score(ground_truth, pred)\n        precision = precision_score(ground_truth, pred)\n        recall = recall_score(ground_truth, pred)\n        f1 = f1_score(ground_truth, pred)\n\n        print(f\"Consumer: {consumer_id}\")\n        print(f\"Accuracy: {accuracy:.2f}\")\n        print(f\"Precision: {precision:.2f}\")\n        print(f\"Recall: {recall:.2f}\")\n        print(f\"F1 Score: {f1:.2f}\")\n        print(\"--------------\")\n\n        total_accuracy.append(accuracy)\n        total_precision.append(precision)\n        total_recall.append(recall)\n        total_f1.append(f1)\n\n        num_time_points = len(features)\n        num_features = features.shape[1]\n\n    print(f'Total accuracy: {sum(total_accuracy)/len(total_accuracy):.4f}', )\n    print(f'Total precision: {sum(total_precision)/len(total_precision):.4f}', )\n    print(f'Total recall: {sum(total_recall)/len(total_recall):.4f}', )\n    print(f'Total f1: {sum(total_f1)/len(total_f1):.4f}', )","block_group":"098221da465549bc838fa5b62e25cef9","execution_count":null,"outputs":[{"name":"stdout","text":"Agglomerative Clustering\n\n\nDataset: 1\nConsumer: 27396\nAccuracy: 0.01\nPrecision: 0.00\nRecall: 0.00\nF1 Score: 0.00\n--------------\nConsumer: 31709\nAccuracy: 0.98\nPrecision: 0.97\nRecall: 0.98\nF1 Score: 0.98\n--------------\nConsumer: 19373\nAccuracy: 0.99\nPrecision: 0.97\nRecall: 1.00\nF1 Score: 0.98\n--------------\nConsumer: 4691\nAccuracy: 0.75\nPrecision: 0.89\nRecall: 0.55\nF1 Score: 0.68\n--------------\nConsumer: 11612\nAccuracy: 0.28\nPrecision: 0.00\nRecall: 0.00\nF1 Score: 0.00\n--------------\nConsumer: 27966\nAccuracy: 0.02\nPrecision: 0.00\nRecall: 0.00\nF1 Score: 0.00\n--------------\nConsumer: 28310\nAccuracy: 0.90\nPrecision: 0.80\nRecall: 0.98\nF1 Score: 0.88\n--------------\nConsumer: 27391\nAccuracy: 0.48\nPrecision: 0.00\nRecall: 0.00\nF1 Score: 0.00\n--------------\nConsumer: 30199\nAccuracy: 0.57\nPrecision: 0.41\nRecall: 0.39\nF1 Score: 0.40\n--------------\nConsumer: 26952\nAccuracy: 0.49\nPrecision: 0.07\nRecall: 0.06\nF1 Score: 0.07\n--------------\nConsumer: 30894\nAccuracy: 0.80\nPrecision: 0.90\nRecall: 0.64\nF1 Score: 0.75\n--------------\nConsumer: 46170\nAccuracy: 0.48\nPrecision: 0.40\nRecall: 0.37\nF1 Score: 0.38\n--------------\nConsumer: 42635\nAccuracy: 0.51\nPrecision: 0.32\nRecall: 0.12\nF1 Score: 0.17\n--------------\nConsumer: 7020\nAccuracy: 0.80\nPrecision: 1.00\nRecall: 0.32\nF1 Score: 0.49\n--------------\nConsumer: 12791\nAccuracy: 0.59\nPrecision: 0.46\nRecall: 0.38\nF1 Score: 0.42\n--------------\nConsumer: 28961\nAccuracy: 0.72\nPrecision: 0.00\nRecall: 0.00\nF1 Score: 0.00\n--------------\nConsumer: 41436\nAccuracy: 0.69\nPrecision: 0.83\nRecall: 0.19\nF1 Score: 0.31\n--------------\nConsumer: 25414\nAccuracy: 0.16\nPrecision: 0.00\nRecall: 0.00\nF1 Score: 0.00\n--------------\nConsumer: 21496\nAccuracy: 0.35\nPrecision: 0.01\nRecall: 0.06\nF1 Score: 0.02\n--------------\nConsumer: 45806\nAccuracy: 0.12\nPrecision: 0.00\nRecall: 0.00\nF1 Score: 0.00\n--------------\nTotal accuracy: 0.5341\nTotal precision: 0.4023\nTotal recall: 0.3017\nTotal f1: 0.3261\nDataset: 2\nConsumer: 27396\nAccuracy: 0.99\nPrecision: 0.94\nRecall: 1.00\nF1 Score: 0.97\n--------------\nConsumer: 19373\nAccuracy: 0.99\nPrecision: 0.89\nRecall: 1.00\nF1 Score: 0.94\n--------------\nConsumer: 10042\nAccuracy: 0.55\nPrecision: 0.00\nRecall: 0.00\nF1 Score: 0.00\n--------------\nConsumer: 21532\nAccuracy: 0.34\nPrecision: 0.00\nRecall: 0.00\nF1 Score: 0.00\n--------------\nConsumer: 18786\nAccuracy: 0.93\nPrecision: 1.00\nRecall: 0.74\nF1 Score: 0.85\n--------------\nConsumer: 46170\nAccuracy: 0.91\nPrecision: 0.89\nRecall: 0.80\nF1 Score: 0.84\n--------------\nConsumer: 28639\nAccuracy: 0.83\nPrecision: 0.00\nRecall: 0.00\nF1 Score: 0.00\n--------------\nConsumer: 41383\nAccuracy: 0.23\nPrecision: 0.00\nRecall: 0.00\nF1 Score: 0.00\n--------------\nConsumer: 48747\nAccuracy: 0.63\nPrecision: 0.13\nRecall: 0.27\nF1 Score: 0.17\n--------------\nConsumer: 36438\nAccuracy: 0.77\nPrecision: 0.56\nRecall: 0.54\nF1 Score: 0.55\n--------------\nConsumer: 34715\nAccuracy: 0.99\nPrecision: 0.94\nRecall: 1.00\nF1 Score: 0.97\n--------------\nConsumer: 24746\nAccuracy: 0.41\nPrecision: 0.01\nRecall: 0.01\nF1 Score: 0.01\n--------------\nConsumer: 46526\nAccuracy: 0.44\nPrecision: 0.42\nRecall: 0.47\nF1 Score: 0.44\n--------------\nConsumer: 18245\nAccuracy: 0.63\nPrecision: 0.05\nRecall: 0.09\nF1 Score: 0.06\n--------------\nConsumer: 8355\nAccuracy: 0.26\nPrecision: 0.12\nRecall: 0.08\nF1 Score: 0.10\n--------------\nConsumer: 21496\nAccuracy: 0.98\nPrecision: 0.96\nRecall: 1.00\nF1 Score: 0.98\n--------------\nConsumer: 14105\nAccuracy: 0.99\nPrecision: 0.91\nRecall: 1.00\nF1 Score: 0.95\n--------------\nConsumer: 6465\nAccuracy: 0.83\nPrecision: 0.00\nRecall: 0.00\nF1 Score: 0.00\n--------------\nConsumer: 27095\nAccuracy: 0.01\nPrecision: 0.00\nRecall: 0.00\nF1 Score: 0.00\n--------------\nConsumer: 6770\nAccuracy: 0.99\nPrecision: 0.64\nRecall: 1.00\nF1 Score: 0.78\n--------------\nTotal accuracy: 0.6092\nTotal precision: 0.4125\nTotal recall: 0.3762\nTotal f1: 0.3787\nDataset: 3\nConsumer: 27396\nAccuracy: 0.02\nPrecision: 0.00\nRecall: 0.00\nF1 Score: 0.00\n--------------\nConsumer: 23361\nAccuracy: 0.30\nPrecision: 0.00\nRecall: 0.00\nF1 Score: 0.00\n--------------\nConsumer: 3832\nAccuracy: 0.60\nPrecision: 0.00\nRecall: 0.00\nF1 Score: 0.00\n--------------\nConsumer: 20985\nAccuracy: 0.25\nPrecision: 0.06\nRecall: 0.09\nF1 Score: 0.07\n--------------\nConsumer: 18786\nAccuracy: 0.60\nPrecision: 0.00\nRecall: 0.00\nF1 Score: 0.00\n--------------\nConsumer: 12636\nAccuracy: 0.98\nPrecision: 0.97\nRecall: 0.99\nF1 Score: 0.98\n--------------\nConsumer: 30894\nAccuracy: 0.56\nPrecision: 0.00\nRecall: 0.00\nF1 Score: 0.00\n--------------\nConsumer: 1318\nAccuracy: 0.32\nPrecision: 0.02\nRecall: 0.13\nF1 Score: 0.03\n--------------\nConsumer: 543\nAccuracy: 0.72\nPrecision: 0.88\nRecall: 0.29\nF1 Score: 0.43\n--------------\nConsumer: 4011\nAccuracy: 0.17\nPrecision: 0.00\nRecall: 0.00\nF1 Score: 0.00\n--------------\nConsumer: 38899\nAccuracy: 0.71\nPrecision: 0.80\nRecall: 0.21\nF1 Score: 0.34\n--------------\nConsumer: 7020\nAccuracy: 0.63\nPrecision: 0.01\nRecall: 0.02\nF1 Score: 0.01\n--------------\nConsumer: 8189\nAccuracy: 0.53\nPrecision: 0.00\nRecall: 0.00\nF1 Score: 0.00\n--------------\nConsumer: 20043\nAccuracy: 0.10\nPrecision: 0.00\nRecall: 0.00\nF1 Score: 0.00\n--------------\nConsumer: 27171\nAccuracy: 0.76\nPrecision: 0.68\nRecall: 0.55\nF1 Score: 0.61\n--------------\nConsumer: 27439\nAccuracy: 0.97\nPrecision: 0.72\nRecall: 1.00\nF1 Score: 0.84\n--------------\nConsumer: 21496\nAccuracy: 0.97\nPrecision: 0.76\nRecall: 0.87\nF1 Score: 0.81\n--------------\nConsumer: 28544\nAccuracy: 0.42\nPrecision: 0.04\nRecall: 0.03\nF1 Score: 0.04\n--------------\nConsumer: 7212\nAccuracy: 0.05\nPrecision: 0.00\nRecall: 0.00\nF1 Score: 0.00\n--------------\nConsumer: 10656\nAccuracy: 0.70\nPrecision: 0.30\nRecall: 0.93\nF1 Score: 0.45\n--------------\nTotal accuracy: 0.5793\nTotal precision: 0.3625\nTotal recall: 0.3358\nTotal f1: 0.3294\nDataset: 4\nConsumer: 25096\nAccuracy: 0.05\nPrecision: 0.02\nRecall: 0.03\nF1 Score: 0.02\n--------------\nConsumer: 19373\nAccuracy: 0.42\nPrecision: 0.00\nRecall: 0.00\nF1 Score: 0.00\n--------------\nConsumer: 23361\nAccuracy: 0.93\nPrecision: 0.87\nRecall: 0.99\nF1 Score: 0.93\n--------------\nConsumer: 29752\nAccuracy: 0.80\nPrecision: 0.07\nRecall: 0.41\nF1 Score: 0.11\n--------------\nConsumer: 27966\nAccuracy: 0.02\nPrecision: 0.00\nRecall: 0.00\nF1 Score: 0.00\n--------------\nConsumer: 1091\nAccuracy: 0.55\nPrecision: 0.17\nRecall: 1.00\nF1 Score: 0.29\n--------------\nConsumer: 18786\nAccuracy: 0.98\nPrecision: 0.97\nRecall: 0.98\nF1 Score: 0.98\n--------------\nConsumer: 42635\nAccuracy: 0.59\nPrecision: 0.18\nRecall: 1.00\nF1 Score: 0.30\n--------------\nConsumer: 543\nAccuracy: 0.27\nPrecision: 0.17\nRecall: 0.44\nF1 Score: 0.24\n--------------\nConsumer: 3799\nAccuracy: 0.01\nPrecision: 0.00\nRecall: 0.00\nF1 Score: 0.00\n--------------\nConsumer: 32895\nAccuracy: 0.02\nPrecision: 0.00\nRecall: 0.00\nF1 Score: 0.00\n--------------\nConsumer: 36438\nAccuracy: 0.85\nPrecision: 0.00\nRecall: 0.00\nF1 Score: 0.00\n--------------\nConsumer: 26637\nAccuracy: 0.70\nPrecision: 0.00\nRecall: 0.00\nF1 Score: 0.00\n--------------\nConsumer: 678\nAccuracy: 0.98\nPrecision: 0.91\nRecall: 1.00\nF1 Score: 0.95\n--------------\nConsumer: 31543\nAccuracy: 0.79\nPrecision: 0.91\nRecall: 0.46\nF1 Score: 0.61\n--------------\nConsumer: 1209\nAccuracy: 0.46\nPrecision: 0.09\nRecall: 0.11\nF1 Score: 0.10\n--------------\nConsumer: 18828\nAccuracy: 0.68\nPrecision: 1.00\nRecall: 0.12\nF1 Score: 0.22\n--------------\nConsumer: 45806\nAccuracy: 0.99\nPrecision: 0.92\nRecall: 1.00\nF1 Score: 0.96\n--------------\nConsumer: 10656\nAccuracy: 0.28\nPrecision: 0.00\nRecall: 0.00\nF1 Score: 0.00\n--------------\nConsumer: 2440\nAccuracy: 0.99\nPrecision: 0.94\nRecall: 1.00\nF1 Score: 0.97\n--------------\nTotal accuracy: 0.5766\nTotal precision: 0.3620\nTotal recall: 0.3587\nTotal f1: 0.3305\nDataset: 5\nConsumer: 19373\nAccuracy: 0.95\nPrecision: 0.92\nRecall: 0.81\nF1 Score: 0.86\n--------------\nConsumer: 31319\nAccuracy: 0.98\nPrecision: 0.89\nRecall: 1.00\nF1 Score: 0.94\n--------------\nConsumer: 10042\nAccuracy: 0.99\nPrecision: 0.92\nRecall: 0.98\nF1 Score: 0.95\n--------------\nConsumer: 38452\nAccuracy: 0.44\nPrecision: 0.34\nRecall: 0.57\nF1 Score: 0.43\n--------------\nConsumer: 26952\nAccuracy: 0.25\nPrecision: 0.01\nRecall: 0.01\nF1 Score: 0.01\n--------------\nConsumer: 1318\nAccuracy: 0.12\nPrecision: 0.11\nRecall: 0.13\nF1 Score: 0.12\n--------------\nConsumer: 32895\nAccuracy: 0.99\nPrecision: 0.83\nRecall: 1.00\nF1 Score: 0.91\n--------------\nConsumer: 7020\nAccuracy: 0.68\nPrecision: 0.00\nRecall: 0.00\nF1 Score: 0.00\n--------------\nConsumer: 49438\nAccuracy: 0.41\nPrecision: 0.31\nRecall: 0.51\nF1 Score: 0.39\n--------------\nConsumer: 678\nAccuracy: 0.98\nPrecision: 0.87\nRecall: 1.00\nF1 Score: 0.93\n--------------\nConsumer: 78\nAccuracy: 0.60\nPrecision: 0.47\nRecall: 0.42\nF1 Score: 0.45\n--------------\nConsumer: 5932\nAccuracy: 0.91\nPrecision: 0.81\nRecall: 0.92\nF1 Score: 0.86\n--------------\nConsumer: 48264\nAccuracy: 0.88\nPrecision: 0.69\nRecall: 0.74\nF1 Score: 0.71\n--------------\nConsumer: 27171\nAccuracy: 0.92\nPrecision: 0.88\nRecall: 0.95\nF1 Score: 0.92\n--------------\nConsumer: 2220\nAccuracy: 0.77\nPrecision: 0.61\nRecall: 0.70\nF1 Score: 0.65\n--------------\nConsumer: 21496\nAccuracy: 0.78\nPrecision: 0.99\nRecall: 0.31\nF1 Score: 0.48\n--------------\nConsumer: 28544\nAccuracy: 0.49\nPrecision: 0.35\nRecall: 0.41\nF1 Score: 0.38\n--------------\nConsumer: 14105\nAccuracy: 0.99\nPrecision: 0.97\nRecall: 1.00\nF1 Score: 0.99\n--------------\nConsumer: 43355\nAccuracy: 0.66\nPrecision: 0.37\nRecall: 1.00\nF1 Score: 0.54\n--------------\nConsumer: 45806\nAccuracy: 0.01\nPrecision: 0.00\nRecall: 0.00\nF1 Score: 0.00\n--------------\nTotal accuracy: 0.5995\nTotal precision: 0.4031\nTotal recall: 0.4115\nTotal f1: 0.3794\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":0},"cell_id":"9de62ed6ec6440369856df1039bd0392","deepnote_cell_type":"text-cell-h2"},"source":"## Isolation Forest + Local Outlier Factor","block_group":"3f9ce83bc3d24473b2994b377762b255"},{"cell_type":"code","metadata":{"source_hash":"a8a852bf","execution_start":1699251750125,"execution_millis":14,"deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":0},"deepnote_to_be_reexecuted":false,"cell_id":"cae3fc9c09c448d895936f89abe2da4d","deepnote_cell_type":"code"},"source":"# Combined models\n\ncombined_pred = np.logical_or(isolation_pred, lof_pred).astype(int)\n\naccuracy = accuracy_score(ground_truth, combined_pred)\nprecision = precision_score(ground_truth, combined_pred)\nrecall = recall_score(ground_truth, combined_pred)\nf1 = f1_score(ground_truth, combined_pred)\n\nprint(f\"Consumer: {consumer_id}\")\nprint(f\"Combined Accuracy: {accuracy:.2f}\")\nprint(f\"Combined Precision: {precision:.2f}\")\nprint(f\"Combined Recall: {recall:.2f}\")\nprint(f\"Combined F1 Score: {f1:.2f}\")\nprint(\"--------------\")\n\ntotal_accuracy.append(accuracy)\ntotal_precision.append(precision)\ntotal_recall.append(recall)\ntotal_f1.append(f1)","block_group":"fbd000c8572f45a7bb125617d9b60b2f","execution_count":null,"outputs":[{"name":"stdout","text":"Consumer: 45806\nCombined Accuracy: 0.58\nCombined Precision: 0.58\nCombined Recall: 0.19\nCombined F1 Score: 0.29\n--------------\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":0},"cell_id":"56a1ffd00f464f33a41931f3a3e765b2","deepnote_cell_type":"text-cell-h2"},"source":"## Isolation Forest + Kmeans","block_group":"b9560d02617e493c8a4f0e4ce8532e53"},{"cell_type":"code","metadata":{"source_hash":"64848221","execution_start":1699251750134,"execution_millis":26,"deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":0},"deepnote_to_be_reexecuted":false,"cell_id":"7aa12d2741df4a0db0ac7354989a0f66","deepnote_cell_type":"code"},"source":"# Combined models\n\ncombined_pred = np.logical_or(isolation_pred, kmeans_pred).astype(int)\n\naccuracy = accuracy_score(ground_truth, combined_pred)\nprecision = precision_score(ground_truth, combined_pred)\nrecall = recall_score(ground_truth, combined_pred)\nf1 = f1_score(ground_truth, combined_pred)\n\nprint(f\"Consumer: {consumer_id}\")\nprint(f\"Combined Accuracy: {accuracy:.2f}\")\nprint(f\"Combined Precision: {precision:.2f}\")\nprint(f\"Combined Recall: {recall:.2f}\")\nprint(f\"Combined F1 Score: {f1:.2f}\")\nprint(\"--------------\")\n\ntotal_accuracy.append(accuracy)\ntotal_precision.append(precision)\ntotal_recall.append(recall)\ntotal_f1.append(f1)","block_group":"4a460aa4019346919269472ee747fbde","execution_count":null,"outputs":[{"name":"stdout","text":"Consumer: 45806\nCombined Accuracy: 0.57\nCombined Precision: 0.54\nCombined Recall: 0.17\nCombined F1 Score: 0.25\n--------------\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":0},"cell_id":"e1e54509fc2344079891fedf0f366798","deepnote_cell_type":"text-cell-h2"},"source":"## Wasserstein + Isolation Forest","block_group":"4b7b165b89e8427ca51825800f3e35cc"},{"cell_type":"code","metadata":{"source_hash":"61e39fc5","execution_start":1699251750151,"execution_millis":204,"deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":0},"deepnote_to_be_reexecuted":false,"cell_id":"67ec9aa5c2374592a6069baafc6fd7e9","deepnote_cell_type":"code"},"source":"# Assuming `results` contains Wasserstein distances for each window\nfeatures_wasserstein = np.array(results).reshape(-1, 1)  # Reshape to use as a feature\n\n# Combine with other features if needed\n# features = np.hstack((features_wasserstein, other_features))\n\n# Normalize the features including Wasserstein distances\nfeatures = scaler.fit_transform(features_wasserstein)\n\n# Apply PCA only if there are more than one feature\nif features.shape[1] > 1:\n    pca = PCA(n_components=min(features.shape[0], features.shape[1]))\n    features = pca.fit_transform(features)\n\n# Train the Isolation Forest on the combined feature set\nisf = IsolationForest(max_samples='auto', random_state=2, n_estimators=50, contamination=float(0.1))\nisf.fit(features)\n\n# Make predictions\npred = isf.predict(features)\n\n# Convert predictions to match ground truth format\npred = np.where(pred == 1, 0, 1)  # Switching -1 to 1 for anomalies and 1 to 0 for normal\n\nif len(pred) > len(ground_truth):\n    pred = pred[:len(ground_truth)]\n\naccuracy = accuracy_score(ground_truth, pred)\nprecision = precision_score(ground_truth, pred)\nrecall = recall_score(ground_truth, pred)\nf1 = f1_score(ground_truth, pred)\n\nprint(f\"Consumer: {consumer_id}\")\nprint(f\"Combined Accuracy: {accuracy:.2f}\")\nprint(f\"Combined Precision: {precision:.2f}\")\nprint(f\"Combined Recall: {recall:.2f}\")\nprint(f\"Combined F1 Score: {f1:.2f}\")\nprint(\"--------------\")\n\n","block_group":"d6266a815ab9422fa03a641cea8b884a","execution_count":null,"outputs":[{"name":"stdout","text":"Consumer: 45806\nCombined Accuracy: 0.50\nCombined Precision: 0.23\nCombined Recall: 0.05\nCombined F1 Score: 0.09\n--------------\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"source_hash":"9fdef2a0","execution_start":1699332198289,"execution_millis":1119,"deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":0},"deepnote_to_be_reexecuted":false,"cell_id":"33372297746243ca97e0099e4ab273cd","deepnote_cell_type":"code"},"source":"def setupXData(consumer_data):\n    day = 96\n    week_period = 7 # number of consecutive days to detect a change\n    ncp = 20 # ncp -> 100 consumers, 20 changes (ncp = 20)\n    year = 96 * 365\n    window_len = day*week_period # window length for the statistical tests\n\n    X_train = [] # set the data of the initial window w0\n    X_test = []\n\n    for i in range(0, year-(7*96), day): \n        X_train.append(consumer_data[i:i+window_len])\n        X_test.append(consumer_data[i+year:(i+year)+window_len])\n\n    X_train = [l.tolist() for l in X_train]\n    X_test = [l.tolist() for l in X_test]\n    return X_train, X_test\n\ndef setupYData(consumer_data, consumer_id, fraud_point):\n    day = 96 \n    week_period = 7 # number of consecutive days to detect a change\n    year = 96 * 365\n    window_len = day*week_period # window length for the statistical tests\n\n    c_train = []\n    c_test = []\n    for i in range(0, year-(7*96), day):  \n        c_train.append(consumer_data[i:i+window_len])\n        c_test.append(consumer_data[i+year:(i+year)+window_len])\n    \n    all_train_data = []\n    for i in c_train:\n        c_train_copy = ','.join(map(str, i))\n        all_train_data.append(c_train_copy)\n        \n    c_train_df = {'ID': consumer_id,\n                  'Consumption': all_train_data,\n                   'Labels': -1}\n    c_train_df = pd.DataFrame(c_train_df)\n    c_train_df.to_csv('./'+ str(consumer_id) + '_train.csv')\n\n    \n    all_test_data = []\n    for i in c_test:\n        c_test_copy = ','.join(map(str, i))\n        all_test_data.append(c_test_copy)\n        \n    c_test_df = {'ID': consumer_id,\n                 'Consumption': all_test_data,\n                   'Labels': -1}\n    c_test_df = pd.DataFrame(c_test_df)\n    fraud_point_df = int((fraud_point - (365*96))/(96*7))\n#     print(fraud_point_df)\n    c_test_df.loc[fraud_point_df:, 'Labels'] = 1\n#     print(c_test_df)\n    c_test_df.to_csv('./'+ str(consumer_id) + '_test.csv')\n\n    y_train = c_train_df\n    y_test = c_test_df\n    return y_train, y_test\n\ntotal_accuracy = []\ntotal_precision = []\ntotal_recall = []\ntotal_f1 = []\ntsf_pred = []\n\nprint('Time Series Forest\\n\\n')\n\ncount = 0\nfor i in all_data:\n    count += 1\n    print(f'Dataset: {count}')\n    for j in range(len(i.index)):\n        consumer_data, consumer_id, fraud_point = consumerData(i, j)\n        fraud_point_week = (fraud_point/96)\n        X_train, X_test = setupXData(consumer_data)\n        y_train, y_test = setupYData(consumer_data, consumer_id, fraud_point)\n\n        tsf = TimeSeriesForestClassifier()\n        tsf.fit(X_train, y_train)\n        pred = tsf.predict(X_test)\n\n\n        # weekly_consumption_year1, weekly_consumption_year2 = setupWeeklyData(consumer_data)\n\n        # features = []\n        # for week_data in weekly_consumption_year1 + weekly_consumption_year2:\n        #     result = catch22_all(week_data)\n        #     features.append(result['values'])\n\n        # features = np.array(features)\n        # features = features[~np.isnan(features).any(axis=1)]\n        # features = scaler.fit_transform(features)\n        # features = pca.fit_transform(features)\n\n        # tsf = TimeSeriesForestClassifier()\n        # tsf.fit(features)\n        # # Predict on the test data\n        # pred = tsf.predict(features)\n\n        # ground_truth = get_ground_truth(len(features), fraud_point_week)\n\n        accuracy = accuracy_score(y_test, pred)\n        precision = precision_score(y_test, pred)\n        recall = recall_score(y_test, pred)\n        f1 = f1_score(y_test, pred)\n\n        print(f\"Consumer: {consumer_id}\")\n        print(f\"Accuracy: {accuracy:.2f}\")\n        print(f\"Precision: {precision:.2f}\")\n        print(f\"Recall: {recall:.2f}\")\n        print(f\"F1 Score: {f1:.2f}\")\n        print(\"--------------\")\n\n        # Assuming total_accuracy, total_precision, total_recall, and total_f1 are lists that have been defined prior to the loop\n        total_accuracy.append(accuracy)\n        total_precision.append(precision)\n        total_recall.append(recall)\n        total_f1.append(f1)\n\n    print(f'Total accuracy: {sum(total_accuracy)/len(total_accuracy):.4f}')\n    print(f'Total precision: {sum(total_precision)/len(total_precision):.4f}')\n    print(f'Total recall: {sum(total_recall)/len(total_recall):.4f}')\n    print(f'Total f1: {sum(total_f1)/len(total_f1):.4f}')","block_group":"91cc5d7213aa40239a0d7f227be0f0c4","execution_count":null,"outputs":[{"name":"stdout","text":"Time Series Forest\n\n\n","output_type":"stream"},{"output_type":"error","ename":"NameError","evalue":"name 'all_data' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn [1], line 71\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime Series Forest\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     70\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[43mall_data\u001b[49m:\n\u001b[1;32m     72\u001b[0m     count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcount\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'all_data' is not defined"]}]},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=d7f56da1-e665-45c2-b1a9-a2c9d93d2a27' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"orig_nbformat":2,"deepnote_app_layout":"powerful-article","deepnote_notebook_id":"43b2ba416b474824b601539ed001342b","deepnote_app_clear_outputs":false,"deepnote_app_table_of_contents_enabled":true,"deepnote_execution_queue":[]}}